{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d9fe0a-d621-4ef4-986c-ab0e6e2feffc",
   "metadata": {},
   "source": [
    "- sentence-transformers\n",
    "    - all-MiniLM-L6-v2: 384\n",
    "    - all-mpnet-base-v2: 768\n",
    "    - The `all-mpnet-base-v2` model provides the best quality, while `all-MiniLM-L6-v2` is 5 times faster and still offers good quality. \n",
    "- openai\n",
    "    - text-embedding-3-small: 1536\n",
    "    - text-embedding-3-large: 3072\n",
    "- llama3\n",
    "    - https://stackoverflow.com/questions/76926025/sentence-embeddings-from-llama-2-huggingface-opensource\n",
    "    - 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f017f8-0e1e-4082-bdf4-d9160b7f6aa0",
   "metadata": {},
   "source": [
    "### mean pooling of llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038555d-7343-44fe-a143-78a0c105b185",
   "metadata": {},
   "source": [
    "```\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = AutoModel.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 生成embeddings\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # outputs.last_hidden_state.shape: [batch_size, seq_len, 4096]\n",
    "        # 使用最后一层的隐藏状态的平均值作为句子嵌入\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "        # embedding.shape: [batch_size, 4096]\n",
    "\n",
    "        embeddings.append(embedding[0].numpy())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629cfcb5-37a5-4d20-8ed0-0a9a95db4427",
   "metadata": {},
   "source": [
    "### prompt-based last token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed509df-fa37-467c-8812-e4b93bd8ecb9",
   "metadata": {},
   "source": [
    "`prompt_template = \"This sentence: {text} means in one word:\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31229783-2155-4973-9ffc-27ed51d19be7",
   "metadata": {},
   "source": [
    "```\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = AutoModel.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add prompt template\n",
    "prompt_template = \"This sentence: {text} means in one word:\"\n",
    "prompted_texts = [prompt_template.format(text=text) for text in texts]\n",
    "\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    # Batch process all texts\n",
    "    inputs = tokenizer(prompted_texts, padding=True, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    \n",
    "    # Get the last hidden state\n",
    "    last_hidden_state = outputs.hidden_states[-1]\n",
    "    \n",
    "    # Get the index of the last non-padding token for each sequence\n",
    "    last_token_indices = inputs.attention_mask.bool().sum(1) - 1\n",
    "    \n",
    "    # Extract embeddings for the last token of each sequence\n",
    "    batch_embeddings = last_hidden_state[torch.arange(last_hidden_state.shape[0]), last_token_indices]\n",
    "    embeddings = batch_embeddings.numpy()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
