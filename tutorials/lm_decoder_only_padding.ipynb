{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5f27c3-a923-47e4-bf05-aebc5e71a891",
   "metadata": {},
   "source": [
    "- https://huggingface.co/docs/transformers/llm_tutorial#wrong-padding-side\n",
    "- decoder-only => left padding\n",
    "    - batch inputs 是会存在 padding 的需求，因为要组织成一个结构化的 tensor\n",
    "    - padding：right，右侧填充，左侧对齐；left：左侧填充，右侧对齐；\n",
    "    - 不仅要设置为 left padding，而且要在 model.generate 的时候要传入 `attention_mask`（不只有 `input_ids`）\n",
    "    - 使用左填充可以将实际数据对齐到右侧，方便模型从左到右处理序列。\n",
    "    - This is because the output is a continuation of the input prompt -- there would be gaps in the output without left padding.\n",
    "- 如果 decoder-only 在generate时，tokenizer.padding_side 被设置为 `right`，Transformer 代码会报警告\n",
    "\n",
    "    ```\n",
    "    A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "    ```\n",
    "- 位置编码（position ids）与 attention mask\n",
    "    - 绝对位置编码，相对位置编码；\n",
    "        - gpt2 是**绝对位置**编码（https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1246C1-L1247C62）\n",
    "        ```\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "        ```\n",
    "    - attention_mask\n",
    "        - GPT2 like decoder only 的 language model，在 generation 的时候，如果是 right padding（右侧padding，左侧对齐），基本都是有问题的，核心在于这种 autoregressive model 在 generation 的时候，是用的 current last hidden state 生成的，而在 input + pads => output 的时候，第一个 output 会用到最后一个 pad 的 hidden state。跟 attention_mask 的关系不大，attention mask 不会做截断，只是在计算 attention weight （softmax之前）+ (-inf)，使得 pad tokens 失效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a0445-a711-4aab-9cd2-b51bf823ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 加载预训练的 GPT-2 模型和 tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58726b95-814f-47ef-b863-7007ec2a9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side, tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f06ab-cea8-4d30-ac5c-61f97994c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9119ae7d-48ae-4fb1-8d74-a065dca164f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输入句子\n",
    "input_text = \"I love you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db94b3-826b-4c41-b9c8-f2a18e537c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码输入句子\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe87d3-18b9-40fd-a385-66b37597590a",
   "metadata": {},
   "source": [
    "## `model.generate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc673751-6ea2-46bc-9bcd-50d23353daab",
   "metadata": {},
   "source": [
    "`# decoder-only models should use left-padding for generation`\n",
    "- inputs\n",
    "    - input_ids\n",
    "    - attention_mask\n",
    "        - position_ids\n",
    "- padding_right 判断的标识\n",
    "    - `torch.sum(input_tensors[:, -1] == generation_config.pad_token_id) > 0`\n",
    "- wte, wpe 都是正常算\n",
    "    - `attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min`\n",
    "        - pad => -inf\n",
    "        - 非pad => 0\n",
    "    - 在算 attention 的时候，attn_weights = attn_weights + attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fdc23cae-7f85-46f7-8fb5-6d066b88ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat([torch.full((1, padding_length), padding_token_id).to(device), input_ids], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa74d5fc-fd1c-44d8-91fa-df1db1335c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_attention_mask = torch.cat([torch.zeros((1, padding_length)), torch.ones(input_ids.shape)], dim=1).to(device)\n",
    "left_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0bd7a39-ccae-41f6-be8b-3c709c8b8a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 1, 2]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = left_attention_mask.long().cumsum(-1) - 1\n",
    "position_ids.masked_fill_(left_attention_mask == 0, 1)\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b5fddf8-edfd-46a7-ab33-45156a3915b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds = model.transformer.wte(input_ids)\n",
    "position_embeds = model.transformer.wpe(position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73bf47c3-00bd-462b-9e9e-50f31884e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.transformer.wpe(torch.tensor([1]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "941b9c94-8d7c-4140-aa13-0f2608733ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = inputs_embeds + position_embeds\n",
    "hidden_states = model.transformer.drop(hidden_states)\n",
    "# seqlen 的前两个位置是 pad_token_id\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c06de26-583e-419a-b0f8-11f6599e6e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码输入句子\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "input_ids = torch.cat([input_ids, torch.full((1, padding_length), padding_token_id).to(device)], dim=1)\n",
    "right_attention_mask = torch.cat([torch.ones(input_ids.shape), torch.zeros((1, padding_length))], dim=1).to(device)\n",
    "right_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b351a788-f8e3-4ce9-a445-14f3590f4579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = right_attention_mask.long().cumsum(-1) - 1\n",
    "position_ids.masked_fill_(right_attention_mask == 0, 1)\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a12f7-7d1f-436c-9053-1dcf1c91b33a",
   "metadata": {},
   "source": [
    "## coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c5c9c-17a4-4c07-bddc-347edfed331a",
   "metadata": {},
   "source": [
    "### no_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c8e180-8f80-4a65-adb0-517055d9383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs_no_padding = model.generate(input_ids, max_length=input_ids.size(1) + 5, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c1df58c-a240-4b00-88e4-726fcdfcd976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love you, and I love you'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_no_padding = tokenizer.decode(outputs_no_padding[0], skip_special_tokens=True)\n",
    "decoded_no_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6ebd0-2133-49f3-94df-5ac840a41367",
   "metadata": {},
   "source": [
    "### left padding no attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88161f8d-8354-41b4-8246-d734fb6e02ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 50256,    40,  1842,   345]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "padding_length = 2\n",
    "left_padded_input_ids = torch.cat([torch.full((1, padding_length), padding_token_id).to(device), input_ids], dim=1)\n",
    "left_padded_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4135d91-e7a6-4a46-b90a-a0ce383ceb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs_left_padding = model.generate(left_padded_input_ids, \n",
    "                                      max_length=left_padded_input_ids.size(1) + 5, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "230c7625-d765-4a37-92e5-62ae949ea11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love you.\\n\\nI love'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs_left_padding[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b797f3b7-e1ef-4897-ba2f-5c7135faef6c",
   "metadata": {},
   "source": [
    "### left padding with attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5b4308d-4959-407f-8c47-afca1514b76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_attention_mask = torch.cat([torch.zeros((1, padding_length)), torch.ones(input_ids.shape)], dim=1).to(device)\n",
    "left_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f02ae3e-b5ef-4ab1-82a4-3c6c04c4481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs_left_padding = model.generate(left_padded_input_ids, attention_mask=left_attention_mask, \n",
    "                                      max_length=left_padded_input_ids.size(1) + 5, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "258d82d1-21ce-4e8b-8a1e-222c630c93eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love you, and I love you'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs_left_padding[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab87d24-0a70-495d-b053-2efeed5c2521",
   "metadata": {},
   "source": [
    "### right padding with attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4390351b-3255-47a1-8801-ef8ffff19ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,  1842,   345, 50256, 50256]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_padded_input_ids = torch.cat([input_ids, torch.full((1, padding_length), padding_token_id).to(device)], dim=1)\n",
    "right_padded_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0893bc40-e67d-4c51-9e5e-c94b280311ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_attention_mask = torch.cat([torch.ones(input_ids.shape), torch.zeros((1, padding_length))], dim=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2af3001b-e2cf-4783-a59e-02221d5a3ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "outputs_right_padding = model.generate(right_padded_input_ids, attention_mask=right_attention_mask, \n",
    "                                       max_length=right_padded_input_ids.size(1) + 5, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02c1fdcb-c671-49e7-9796-4975d8e48e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love youThe best thing about this'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs_right_padding[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ad751-b16f-4e11-9dd4-37db1deba3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
