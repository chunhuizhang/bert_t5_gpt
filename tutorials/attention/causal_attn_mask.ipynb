{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd25e64-cb3b-4abc-8f19-e201c7237106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1402429-03d7-4907-b141-7232e1a322fe",
   "metadata": {},
   "source": [
    "- GQA 允许使用比 query head 数量更少的 key/value head，以节省计算和内存\n",
    "    - N_q = 4, N_kv = 2\n",
    "        - llama3: `N_q = 32, N_kv = 8`\n",
    "    - key 和 value head 会被复制以匹配 query head 的数量。\n",
    "        - 在 GQA 中，查询头被分成若干组，每组查询头共享同一组键 (Key) 和值 (Value) 头。\n",
    "        - N_q 必须是 N_kv 的整数倍。例如，如果你有 8 个查询头 (N_q=8) 和 2 个键/值头 (N_kv=2)，那么每 4 个查询头会共享同一组键/值头。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf6ba9-aaf8-49b2-b841-1a868a9da413",
   "metadata": {},
   "source": [
    "### padding vs. packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505a0013-c62e-4453-9418-11770b97d525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/packing_padding.jpeg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/packing_padding.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4001d78d-6298-4b62-b76e-51edd389c973",
   "metadata": {},
   "source": [
    "\n",
    "- 左侧是传统的padding做法，右侧是packing，其中红色部分代表pad token，黄色部分代表sep token。\n",
    "    - 都是整理成 batch tensor\n",
    "    - 为了区分不同的训练示例，我们在不同示例之间加上一个分割标记sep token，\n",
    "- **注意力窗口不允许跨示例**。\n",
    "    - padding：传统的全局下三角矩阵。\n",
    "    - packing：这个注意力模式叫块对角矩阵（BlockDiagonalMask）【本质上是在示例内的下三角矩阵】，\n",
    "        - 由此，就消除了对pad token的需要，所以开源大模型刚问世的时候（2023-3那阵子），存在很多base model放出来的tokenizer并没有pad token，比如llama-base。\n",
    "        - 需要注意，packing时示例3可能会被截断，这个行为在预训练时是可以接受的。注意，这个时候的学习模式非常的简单，就是next token prediction。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb013018-c182-4a87-833d-0aaf4a3e1779",
   "metadata": {},
   "source": [
    "### sdpa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9254bd-3972-4742-88e6-655450220de4",
   "metadata": {},
   "source": [
    "- https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f984d1-51c6-443f-8b31-56247dc5898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5831fe08-bb3a-4b10-8d0e-5caa52709656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7698ac1b3390>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1d29d1-19ae-441c-a557-4b805c7374ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: (B, N_q, L, D_k)\n",
    "# key: (B, N_kv, S, D_k)\n",
    "# value: (B, N_kv, S, D_v)\n",
    "query = torch.rand(2, 4, 3, 2, dtype=torch.float16, device=\"cuda\")\n",
    "key = torch.rand(2, 4, 3, 2, dtype=torch.float16, device=\"cuda\")\n",
    "value = torch.rand(2, 4, 3, 2, dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "output = F.scaled_dot_product_attention(query, key, value, is_causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfbdc1b0-f9df-49f8-a8a9-962a37ba1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307e62ba-06be-4d4c-8dc6-58d4f040d95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, S = query.size(-2), key.size(-2)\n",
    "L, S, query.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf1140a-9725-4d6c-aadc-7ba8b03fc221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_factor = 1 / math.sqrt(query.size(-1))\n",
    "scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3382605f-87dc-4c02-96f0-7952c32bca0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ True, False, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True,  True]], device='cuda:0'),\n",
       " tensor([[0., -inf, -inf],\n",
       "         [0., 0., -inf],\n",
       "         [0., 0., 0.]], device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
    "temp_mask = torch.ones(L, S, dtype=torch.bool, device=query.device).tril(diagonal=0)\n",
    "attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "attn_bias.to(query.dtype)\n",
    "temp_mask, attn_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9073152e-e130-4db8-a087-8efa6232155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "\n",
    "# 通过加一个上三角为 -inf 的下三角为 0，然后 softmax 实现 causal mask\n",
    "attn_weight += attn_bias\n",
    "attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "attn_weight = torch.dropout(attn_weight, 0., train=True)\n",
    "output1 = attn_weight @ value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c8c75-ca14-4bee-9c50-23859b3637a1",
   "metadata": {},
   "source": [
    "### enbale gqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6a77c0-d300-4c8c-954f-a02a8419269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: (B, N_q, L, D_k)\n",
    "# key: (B, N_kv, S, D_k)\n",
    "# value: (B, N_kv, S, D_v)\n",
    "query = torch.rand(2, 4, 3, 2, dtype=torch.float16, device=\"cuda\")\n",
    "key = torch.rand(2, 2, 3, 2, dtype=torch.float16, device=\"cuda\")\n",
    "value = torch.rand(2, 2, 3, 2, dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "output = F.scaled_dot_product_attention(query, key, value, enable_gqa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6543de2-0fc7-4bfa-82d1-63206cabc41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
