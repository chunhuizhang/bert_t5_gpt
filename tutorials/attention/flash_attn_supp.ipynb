{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a61512-1159-40b8-97bf-cce295abfdf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:23:04.137824Z",
     "iopub.status.busy": "2024-07-07T09:23:04.137225Z",
     "iopub.status.idle": "2024-07-07T09:23:13.891559Z",
     "shell.execute_reply": "2024-07-07T09:23:13.889735Z",
     "shell.execute_reply.started": "2024-07-07T09:23:04.137779Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "import os\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720bae72-bee5-4c49-875a-0669f343e85e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf {QK}^T\\in \\mathbb R^{N\\times N}\n",
    "$$\n",
    "\n",
    "- 40 heads, bfloat16\n",
    "    - multi head 可以并行地去算；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e77c6-623a-4ae7-b3ff-d5829eb86046",
   "metadata": {},
   "source": [
    "### Multi head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396acbaf-08a9-498b-be03-2a057de867a0",
   "metadata": {},
   "source": [
    "- $h$ 个 head, 每个头的维度为\n",
    "\n",
    "    $$\n",
    "    d_k=d_v=\\frac{d_{model}}{h}\n",
    "    $$\n",
    "\n",
    "\n",
    "- 线性变换生成多头的 $Q_i, K_i, V_i$ (for $i=1,2,\\cdots,h$)\n",
    "\n",
    "    $$\n",
    "    Q_i=XW_i^Q, K_i=XW_i^K, V_i=XW_i^V\\quad \\in \\mathbb R^{N\\times\\frac{d_{model}}{h}}\n",
    "    $$\n",
    "\n",
    "- 计算每个头的注意力\n",
    "\n",
    "    $$\n",
    "    \\text{head}_i=\\text{Attention}(Q_i,K_i,V_i)=\\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i\n",
    "    $$\n",
    "  - 尤其需要注意的是 $Q_iK_i^T\\in \\mathbb R^{N\\times N}$\n",
    "- 拼接多头的输出: $\\frac{d_{model}}{h}\\times h$\n",
    "\n",
    "    $$\n",
    "    \\text{MultiHead}(Q,K,V)=\\text{Concat}(head_1, head_2,  \\cdots, head_h)W^o\n",
    "    $$\n",
    "  - $W^o\\in \\mathbb R^{d_{model}\\times d_{model}}$, learnable projection matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba37ac-61c2-490c-adbc-1fdd402657d8",
   "metadata": {},
   "source": [
    "### 空间存储"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912070eb-5f93-4156-b5fd-7e944d3d38d8",
   "metadata": {},
   "source": [
    "- VRAM\n",
    "- O(N^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374fb9e0-68b9-444e-9d23-1e73d2fbaefe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T03:15:23.130653Z",
     "iopub.status.busy": "2024-07-07T03:15:23.130044Z",
     "iopub.status.idle": "2024-07-07T03:15:23.141862Z",
     "shell.execute_reply": "2024-07-07T03:15:23.139964Z",
     "shell.execute_reply.started": "2024-07-07T03:15:23.130607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.2939453125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N = 1000\n",
    "# MB\n",
    "40 * 2 * (1000 ** 2) / (1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8be2ead-3fdc-4fe6-b165-d21fc1360aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T03:15:27.380054Z",
     "iopub.status.busy": "2024-07-07T03:15:27.379491Z",
     "iopub.status.idle": "2024-07-07T03:15:27.391749Z",
     "shell.execute_reply": "2024-07-07T03:15:27.389756Z",
     "shell.execute_reply.started": "2024-07-07T03:15:27.380010Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.073486328125"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N = 16000\n",
    "# GB\n",
    "40 * 2 * (16000 ** 2) / (1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde96dea-187b-40cf-99e6-34d3442f1f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T03:18:34.009853Z",
     "iopub.status.busy": "2024-07-07T03:18:34.009235Z",
     "iopub.status.idle": "2024-07-07T03:18:34.020222Z",
     "shell.execute_reply": "2024-07-07T03:18:34.018540Z",
     "shell.execute_reply.started": "2024-07-07T03:18:34.009807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7275957614183426"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N = 100,000\n",
    "# TB\n",
    "40 * 2 * (100_000 ** 2) / (1024*1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cacbbf90-0320-4ef0-8f16-1e6d5a19fc4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:23:49.986952Z",
     "iopub.status.busy": "2024-07-07T09:23:49.986690Z",
     "iopub.status.idle": "2024-07-07T09:23:49.992276Z",
     "shell.execute_reply": "2024-07-07T09:23:49.991224Z",
     "shell.execute_reply.started": "2024-07-07T09:23:49.986937Z"
    }
   },
   "outputs": [],
   "source": [
    "def bytes_to_giga_bytes(bytes):\n",
    "    return bytes / 1024 / 1024 / 1024\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe4d6ee-9229-4bd9-91ee-9d2d7cc8cd6c",
   "metadata": {},
   "source": [
    "### performance benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6103e3d-a293-4c8b-a6b2-866218c567a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:52:32.761144Z",
     "iopub.status.busy": "2024-07-07T08:52:32.760816Z",
     "iopub.status.idle": "2024-07-07T08:52:41.609358Z",
     "shell.execute_reply": "2024-07-07T08:52:41.608221Z",
     "shell.execute_reply.started": "2024-07-07T08:52:32.761123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901e397fd58248f59096e979a77ed4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", \n",
    "                                             torch_dtype=torch.bfloat16, \n",
    "                                             device_map=\"auto\", \n",
    "                                             pad_token_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc0157c-ef63-4d68-9b22-9399cc4ad094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:52:42.750161Z",
     "iopub.status.busy": "2024-07-07T08:52:42.749649Z",
     "iopub.status.idle": "2024-07-07T08:52:42.764380Z",
     "shell.execute_reply": "2024-07-07T08:52:42.763040Z",
     "shell.execute_reply.started": "2024-07-07T08:52:42.750124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTBigCodeForCausalLM(\n",
       "  (transformer): GPTBigCodeModel(\n",
       "    (wte): Embedding(49152, 6144)\n",
       "    (wpe): Embedding(8192, 6144)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-39): 40 x GPTBigCodeBlock(\n",
       "        (ln_1): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTBigCodeSdpaAttention(\n",
       "          (c_attn): Linear(in_features=6144, out_features=6400, bias=True)\n",
       "          (c_proj): Linear(in_features=6144, out_features=6144, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTBigCodeMLP(\n",
       "          (c_fc): Linear(in_features=6144, out_features=24576, bias=True)\n",
       "          (c_proj): Linear(in_features=24576, out_features=6144, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=6144, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa9c61d0-4103-43ee-8344-0288666129d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:45:44.079300Z",
     "iopub.status.busy": "2024-07-07T08:45:44.078964Z",
     "iopub.status.idle": "2024-07-07T08:45:44.612229Z",
     "shell.execute_reply": "2024-07-07T08:45:44.610899Z",
     "shell.execute_reply.started": "2024-07-07T08:45:44.079279Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ebc73d0-7ca4-40d0-8dcf-abda5c1104bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:23:27.765802Z",
     "iopub.status.busy": "2024-07-07T08:23:27.765320Z",
     "iopub.status.idle": "2024-07-07T08:23:27.774211Z",
     "shell.execute_reply": "2024-07-07T08:23:27.771962Z",
     "shell.execute_reply.started": "2024-07-07T08:23:27.765775Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940cc301-078f-4434-af89-eb61f4f6348e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:45:11.995698Z",
     "iopub.status.busy": "2024-07-07T08:45:11.994106Z",
     "iopub.status.idle": "2024-07-07T08:45:12.003555Z",
     "shell.execute_reply": "2024-07-07T08:45:12.001391Z",
     "shell.execute_reply.started": "2024-07-07T08:45:11.995635Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "327b359a-ef71-4f97-a8bd-7c446b24bdd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:23:58.886247Z",
     "iopub.status.busy": "2024-07-07T08:23:58.885604Z",
     "iopub.status.idle": "2024-07-07T08:24:03.121153Z",
     "shell.execute_reply": "2024-07-07T08:24:03.120367Z",
     "shell.execute_reply.started": "2024-07-07T08:23:58.886199Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Here is a Python function that transforms bytes to Giga bytes:\\n\\n```python\\ndef bytes_to_gigabytes(bytes):\\n    return bytes / 1024 / 1024 / 1024\\n```\\n\\nThis function takes a single'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a26df022-ca6e-4148-b7a6-3919275d9f10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:26:16.209251Z",
     "iopub.status.busy": "2024-07-07T08:26:16.208933Z",
     "iopub.status.idle": "2024-07-07T08:26:16.216702Z",
     "shell.execute_reply": "2024-07-07T08:26:16.214802Z",
     "shell.execute_reply.started": "2024-07-07T08:26:16.209229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a Python function that transforms bytes to Giga bytes:\n",
      "\n",
      "```python\n",
      "def bytes_to_gigabytes(bytes):\n",
      "    return bytes / 1024 / 1024 / 1024\n",
      "```\n",
      "\n",
      "This function takes a single\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d69452bb-39b8-4d23-b0f9-ee734614c1fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:28:28.314445Z",
     "iopub.status.busy": "2024-07-07T08:28:28.314036Z",
     "iopub.status.idle": "2024-07-07T08:28:28.325742Z",
     "shell.execute_reply": "2024-07-07T08:28:28.323899Z",
     "shell.execute_reply.started": "2024-07-07T08:28:28.314416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.97451877593994"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes_to_giga_bytes(torch.cuda.max_memory_allocated(device='cuda:0')) + bytes_to_giga_bytes(torch.cuda.max_memory_allocated(device='cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1597fefa-5c27-4fd8-924c-a2b2c4dcb269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:40:52.697153Z",
     "iopub.status.busy": "2024-07-07T08:40:52.696495Z",
     "iopub.status.idle": "2024-07-07T08:40:52.706868Z",
     "shell.execute_reply": "2024-07-07T08:40:52.704661Z",
     "shell.execute_reply.started": "2024-07-07T08:40:52.697106Z"
    }
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896c5562-baf9-4743-89bd-ad5eabd47910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:51:43.912469Z",
     "iopub.status.busy": "2024-07-07T08:51:43.911840Z",
     "iopub.status.idle": "2024-07-07T08:51:43.921808Z",
     "shell.execute_reply": "2024-07-07T08:51:43.919697Z",
     "shell.execute_reply.started": "2024-07-07T08:51:43.912389Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "733394cd-feba-4bbe-91ed-945344411f85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:53:01.140289Z",
     "iopub.status.busy": "2024-07-07T08:53:01.139647Z",
     "iopub.status.idle": "2024-07-07T08:53:01.336339Z",
     "shell.execute_reply": "2024-07-07T08:53:01.334529Z",
     "shell.execute_reply.started": "2024-07-07T08:53:01.140243Z"
    }
   },
   "outputs": [],
   "source": [
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23611de0-0a90-411b-bb88-f89b8caefea2",
   "metadata": {},
   "source": [
    "### quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef440a5d-f708-48dc-a53f-f3ee3035ab5b",
   "metadata": {},
   "source": [
    "- `bitsandbytes`\n",
    "    - `load_in_8bit=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eed40da-efa5-4cfa-ba47-6812086903ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:50:52.736064Z",
     "iopub.status.busy": "2024-07-07T08:50:52.735605Z",
     "iopub.status.idle": "2024-07-07T08:51:03.118657Z",
     "shell.execute_reply": "2024-07-07T08:51:03.117789Z",
     "shell.execute_reply.started": "2024-07-07T08:50:52.736040Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b566ea8a094c9c917f6934437b0036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec1cd5a9-1dad-4fbb-acce-eec0537aa1e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:51:17.986758Z",
     "iopub.status.busy": "2024-07-07T08:51:17.986508Z",
     "iopub.status.idle": "2024-07-07T08:51:18.491411Z",
     "shell.execute_reply": "2024-07-07T08:51:18.490041Z",
     "shell.execute_reply.started": "2024-07-07T08:51:17.986743Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd10a484-7535-41bc-bfa4-1d926833488f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:44:48.108889Z",
     "iopub.status.busy": "2024-07-07T08:44:48.108183Z",
     "iopub.status.idle": "2024-07-07T08:44:48.117686Z",
     "shell.execute_reply": "2024-07-07T08:44:48.115581Z",
     "shell.execute_reply.started": "2024-07-07T08:44:48.108841Z"
    }
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a42e43-3892-4c66-a7ad-db466a190122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:51:05.773866Z",
     "iopub.status.busy": "2024-07-07T08:51:05.773657Z",
     "iopub.status.idle": "2024-07-07T08:51:05.778574Z",
     "shell.execute_reply": "2024-07-07T08:51:05.777289Z",
     "shell.execute_reply.started": "2024-07-07T08:51:05.773852Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c4c51b-9897-451b-b917-08622ea1a302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:51:21.838369Z",
     "iopub.status.busy": "2024-07-07T08:51:21.838055Z",
     "iopub.status.idle": "2024-07-07T08:51:27.699956Z",
     "shell.execute_reply": "2024-07-07T08:51:27.699091Z",
     "shell.execute_reply.started": "2024-07-07T08:51:21.838350Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a Python function that transforms bytes to Giga bytes:\n",
      "\n",
      "```python\n",
      "def bytes_to_gigabytes(bytes):\n",
      "    return bytes / 1024 / 1024 / 1024\n",
      "```\n",
      "\n",
      "This function takes a single\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "result = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb2d6e94-150f-43a1-a61e-2d8efcfb173a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:51:48.396080Z",
     "iopub.status.busy": "2024-07-07T08:51:48.395442Z",
     "iopub.status.idle": "2024-07-07T08:51:48.410352Z",
     "shell.execute_reply": "2024-07-07T08:51:48.408187Z",
     "shell.execute_reply.started": "2024-07-07T08:51:48.396034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.230342864990234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes_to_giga_bytes(torch.cuda.max_memory_allocated('cuda:0')) + bytes_to_giga_bytes(torch.cuda.max_memory_allocated('cuda:1')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02ebf384-c654-443b-b08a-f219a9baf69f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:51:50.622022Z",
     "iopub.status.busy": "2024-07-07T08:51:50.621386Z",
     "iopub.status.idle": "2024-07-07T08:51:50.633888Z",
     "shell.execute_reply": "2024-07-07T08:51:50.631925Z",
     "shell.execute_reply.started": "2024-07-07T08:51:50.621976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes_to_giga_bytes(torch.cuda.max_memory_allocated('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6b1dede-e9d2-425d-8206-4258e2dd8672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:51:54.274138Z",
     "iopub.status.busy": "2024-07-07T08:51:54.273504Z",
     "iopub.status.idle": "2024-07-07T08:51:54.282833Z",
     "shell.execute_reply": "2024-07-07T08:51:54.280717Z",
     "shell.execute_reply.started": "2024-07-07T08:51:54.274092Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c83d2563-d588-4f7c-ae9a-93d426d84d33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T08:51:56.599638Z",
     "iopub.status.busy": "2024-07-07T08:51:56.598975Z",
     "iopub.status.idle": "2024-07-07T08:51:57.045918Z",
     "shell.execute_reply": "2024-07-07T08:51:57.044565Z",
     "shell.execute_reply.started": "2024-07-07T08:51:56.599590Z"
    }
   },
   "outputs": [],
   "source": [
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7dfff-7df0-41d3-bc72-f12e48d438d3",
   "metadata": {},
   "source": [
    "### flash attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef01b0-5170-41bb-a6f6-22019c90a96e",
   "metadata": {},
   "source": [
    "- outer loop: $\\mathbf K_j, \\mathbf V_j$\n",
    "- inner loop: $\\mathbf Q_i, \\mathbf O_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b27c2d1-e2ee-49ee-a7a2-b2af4cb04ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T03:24:54.099811Z",
     "iopub.status.busy": "2024-07-07T03:24:54.099226Z",
     "iopub.status.idle": "2024-07-07T03:24:54.110922Z",
     "shell.execute_reply": "2024-07-07T03:24:54.108891Z",
     "shell.execute_reply.started": "2024-07-07T03:24:54.099765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/hf_flash_attn.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/v4.35.0/llm_tutorial_optimization#2-flash-attention\n",
    "Image(url='./imgs/hf_flash_attn.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b83342-504a-4a97-a014-0cb5d04f90c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:23:14.490374Z",
     "iopub.status.busy": "2024-07-07T09:23:14.489804Z",
     "iopub.status.idle": "2024-07-07T09:23:14.500824Z",
     "shell.execute_reply": "2024-07-07T09:23:14.498524Z",
     "shell.execute_reply.started": "2024-07-07T09:23:14.490342Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Below are a series of dialogues between various people and an AI technical assistant.\n",
    "The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.\n",
    "The assistant is happy to help with code questions and will do their best to understand exactly what is needed.\n",
    "It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.\n",
    "That said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.\n",
    "\n",
    "The Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).\n",
    "The model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.\n",
    "\n",
    "-----\n",
    "\n",
    "Question: Write a function that takes two lists and returns a list that has alternating elements from each input list.\n",
    "\n",
    "Answer: Sure. Here is a function that does that.\n",
    "\n",
    "def alternating(list1, list2):\n",
    "   results = []\n",
    "   for i in range(len(list1)):\n",
    "       results.append(list1[i])\n",
    "       results.append(list2[i])\n",
    "   return results\n",
    "\n",
    "Question: Can you write some test cases for this function?\n",
    "\n",
    "Answer: Sure, here are some tests.\n",
    "\n",
    "assert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]\n",
    "assert alternating([True, False], [4, 5]) == [True, 4, False, 5]\n",
    "assert alternating([], []) == []\n",
    "\n",
    "Question: Modify the function so that it returns all input elements when the lists have uneven length. The elements from the longer list should be at the end.\n",
    "\n",
    "Answer: Here is the modified function.\n",
    "\n",
    "def alternating(list1, list2):\n",
    "   results = []\n",
    "   for i in range(min(len(list1), len(list2))):\n",
    "       results.append(list1[i])\n",
    "       results.append(list2[i])\n",
    "   if len(list1) > len(list2):\n",
    "       results.extend(list1[i+1:])\n",
    "   else:\n",
    "       results.extend(list2[i+1:])\n",
    "   return results\n",
    "\n",
    "-----\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aec231f-19c6-4f79-be55-bc53226bf5b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:23:19.227312Z",
     "iopub.status.busy": "2024-07-07T09:23:19.226665Z",
     "iopub.status.idle": "2024-07-07T09:23:19.235977Z",
     "shell.execute_reply": "2024-07-07T09:23:19.233785Z",
     "shell.execute_reply.started": "2024-07-07T09:23:19.227267Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2724e762-0b21-4254-8eb8-ae9f838312b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:23:20.565748Z",
     "iopub.status.busy": "2024-07-07T09:23:20.565130Z",
     "iopub.status.idle": "2024-07-07T09:23:20.574307Z",
     "shell.execute_reply": "2024-07-07T09:23:20.572179Z",
     "shell.execute_reply.started": "2024-07-07T09:23:20.565705Z"
    }
   },
   "outputs": [],
   "source": [
    "long_prompt = 10 * system_prompt + prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0890652c-d24b-4c6b-b3bf-de9f2a404807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:23:21.797345Z",
     "iopub.status.busy": "2024-07-07T09:23:21.796709Z",
     "iopub.status.idle": "2024-07-07T09:23:31.922985Z",
     "shell.execute_reply": "2024-07-07T09:23:31.922122Z",
     "shell.execute_reply.started": "2024-07-07T09:23:21.797301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e55c208e8c248458bd3709ec2e2ddc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351ff058-5497-43b5-8e33-b4c3babbad15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:23:37.369713Z",
     "iopub.status.busy": "2024-07-07T09:23:37.369416Z",
     "iopub.status.idle": "2024-07-07T09:23:42.635445Z",
     "shell.execute_reply": "2024-07-07T09:23:42.634589Z",
     "shell.execute_reply.started": "2024-07-07T09:23:37.369696Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated in 5.260891437530518 seconds.\n",
      " Sure, here is a function that does that.\n",
      "\n",
      "def bytes_to_giga(bytes):\n",
      "   return bytes / 1024 / 1024 / 1024\n",
      "\n",
      "Answer: Sure, here is a function that does that.\n",
      "\n",
      "def\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n",
    "\n",
    "print(f\"Generated in {time.time() - start_time} seconds.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccbb5f93-1f5f-4ab0-bd5c-48ea1a44a814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:23:53.940289Z",
     "iopub.status.busy": "2024-07-07T09:23:53.938753Z",
     "iopub.status.idle": "2024-07-07T09:23:53.952197Z",
     "shell.execute_reply": "2024-07-07T09:23:53.950076Z",
     "shell.execute_reply.started": "2024-07-07T09:23:53.940230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.42166042327881"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes_to_giga_bytes(torch.cuda.max_memory_allocated('cuda:0')) + bytes_to_giga_bytes(torch.cuda.max_memory_allocated('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95878d99-c18d-4e59-a871-50b05d1cc348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:23:55.641819Z",
     "iopub.status.busy": "2024-07-07T09:23:55.641207Z",
     "iopub.status.idle": "2024-07-07T09:23:55.900343Z",
     "shell.execute_reply": "2024-07-07T09:23:55.898986Z",
     "shell.execute_reply.started": "2024-07-07T09:23:55.641775Z"
    }
   },
   "outputs": [],
   "source": [
    "flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3b9b3a5-8cfc-45e1-8a18-347f45fbf5ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:07:14.369015Z",
     "iopub.status.busy": "2024-07-07T09:07:14.368259Z",
     "iopub.status.idle": "2024-07-07T09:07:14.378172Z",
     "shell.execute_reply": "2024-07-07T09:07:14.376260Z",
     "shell.execute_reply.started": "2024-07-07T09:07:14.368966Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8504f68-1ea7-4870-b502-dcfa1e9daa40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:09:58.818549Z",
     "iopub.status.busy": "2024-07-07T09:09:58.818021Z",
     "iopub.status.idle": "2024-07-07T09:09:58.827015Z",
     "shell.execute_reply": "2024-07-07T09:09:58.824765Z",
     "shell.execute_reply.started": "2024-07-07T09:09:58.818514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transformers now supports natively BetterTransformer optimizations (torch.nn.functional.scaled_dot_product_attention) for the model type gpt_bigcode. As such, there is no need to use `model.to_bettertransformers()` or `BetterTransformer.transform(model)` from the Optimum library. Please upgrade to transformers>=4.36 and torch>=2.1.1 to use it. \n",
    "# model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "446d3ce8-2fb7-40f0-90c2-c15cd943dcd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:25:34.174158Z",
     "iopub.status.busy": "2024-07-07T09:25:34.173662Z",
     "iopub.status.idle": "2024-07-07T09:25:34.183393Z",
     "shell.execute_reply": "2024-07-07T09:25:34.181349Z",
     "shell.execute_reply.started": "2024-07-07T09:25:34.174123Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['TORCH_CUDNN_SDPA_ENABLED'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f7758a-c1ac-40e6-91ed-7b717eb4064b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:25:35.160507Z",
     "iopub.status.busy": "2024-07-07T09:25:35.159879Z",
     "iopub.status.idle": "2024-07-07T09:25:35.597374Z",
     "shell.execute_reply": "2024-07-07T09:25:35.595168Z",
     "shell.execute_reply.started": "2024-07-07T09:25:35.160461Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/whaow/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:557: UserWarning: Memory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:607.)\n",
      "  sdpa_result = torch.nn.functional.scaled_dot_product_attention(\n",
      "/home/whaow/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:557: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:495.)\n",
      "  sdpa_result = torch.nn.functional.scaled_dot_product_attention(\n",
      "/home/whaow/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:557: UserWarning: Flash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:609.)\n",
      "  sdpa_result = torch.nn.functional.scaled_dot_product_attention(\n",
      "/home/whaow/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:557: UserWarning: Both fused kernels do not support non-null attn_mask. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:269.)\n",
      "  sdpa_result = torch.nn.functional.scaled_dot_product_attention(\n",
      "/home/whaow/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:557: UserWarning: CuDNN attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:611.)\n",
      "  sdpa_result = torch.nn.functional.scaled_dot_product_attention(\n",
      "/home/whaow/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:557: UserWarning: The CuDNN backend needs to be enabled by setting the enviornment variable`TORCH_CUDNN_SDPA_ENABLED=1` (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:409.)\n",
      "  sdpa_result = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No available kernel. Aborting execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SDPBackend, sdpa_kernel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sdpa_kernel(SDPBackend\u001b[38;5;241m.\u001b[39mFLASH_ATTENTION):\n\u001b[0;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlong_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mlen\u001b[39m(long_prompt):]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/pipelines/text_generation.py:240\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/pipelines/base.py:1206\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1199\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1200\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         )\n\u001b[1;32m   1204\u001b[0m     )\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/pipelines/base.py:1213\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1212\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1213\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/pipelines/base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1111\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1112\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/pipelines/text_generation.py:327\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/generation/utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1510\u001b[0m         input_ids,\n\u001b[1;32m   1511\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1524\u001b[0m     )\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/generation/utils.py:2411\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2408\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2411\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:1245\u001b[0m, in \u001b[0;36mGPTBigCodeForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03mlabels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1245\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1260\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1262\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:1098\u001b[0m, in \u001b[0;36mGPTBigCodeModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1087\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1088\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1095\u001b[0m         output_attentions,\n\u001b[1;32m   1096\u001b[0m     )\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1098\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1109\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:713\u001b[0m, in \u001b[0;36mGPTBigCodeBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    711\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    712\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 713\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    722\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:625\u001b[0m, in \u001b[0;36mGPTBigCodeSdpaAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    620\u001b[0m key, value \u001b[38;5;241m=\u001b[39m key_value\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions \u001b[38;5;129;01mand\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;66;03m# Difference with the original implementation: there is no need to transpose the key here,\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;66;03m# as SDPA expects seq_length to be at index -2 for the key as well\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTBigCodeModel is using GPTBigCodeSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` and `head_mask` not None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` when loading the model.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/workspaces/opensource/transformers/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:557\u001b[0m, in \u001b[0;36mGPTBigCodeSdpaAttention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    554\u001b[0m         key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    555\u001b[0m         value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 557\u001b[0m sdpa_result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_pdrop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The query_length > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case query_length == 1.\u001b[39;49;00m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_query:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# (batch_size, num_heads, seq_len, head_dim) --> (batch_size, seq_len, num_heads, head_dim)\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     sdpa_result \u001b[38;5;241m=\u001b[39m sdpa_result\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No available kernel. Aborting execution."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "    result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n",
    "\n",
    "# with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "#     result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n",
    "\n",
    "print(f\"Generated in {time.time() - start_time} seconds.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abfe47c-efef-4f49-93b3-701491c425e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
