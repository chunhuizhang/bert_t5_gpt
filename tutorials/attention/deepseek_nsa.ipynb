{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7aecb5f-acac-4bb0-bbd8-0719870ae044",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/24841366485"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f268a8a-d586-4589-a876-48a2b65c66b7",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be795d5-32ef-47f5-abc2-02ffa723f7c5",
   "metadata": {},
   "source": [
    "- cuda kernel vs. triton kernel\n",
    "- KVcache\n",
    "    - GQA/MQA: key-value caches are shared cross query heads\n",
    "    - NSA实质的Attention计算是GQA\n",
    "- notions\n",
    "    - $h$: head index, $H$: the number of query heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8e3b7-289c-4258-a7f5-ea5a1b176f6f",
   "metadata": {},
   "source": [
    "## algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db42b9-6fcb-41f4-82af-6cc2314ad6d8",
   "metadata": {},
   "source": [
    "### token compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ba227-23d5-46a1-b632-d4325c445ff8",
   "metadata": {},
   "source": [
    "- $\\ell$: block size, $d$: stride\n",
    "    - $t=9$, $\\ell = 3$, $d=2$\n",
    "        - $M=\\lfloor\\frac{t-\\ell}{d}\\rfloor=3$\n",
    "        - [1, 2, 3], [3, 4, 5], [5, 6, 7], [7, 8, 9]\n",
    "    - $\\ell=d$ 时，无重叠；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49ecfa-9671-47b5-8e53-b6ecac542d0f",
   "metadata": {},
   "source": [
    "### token selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b3a81a-642d-4284-b3f1-ee6aae8ae569",
   "metadata": {},
   "source": [
    "> Selection Attention才是真正体现了Sparse的精髓。\n",
    "\n",
    "\n",
    "$$\n",
    "p_t^{cmp}=\\text{Softmax}(q_t^T\\tilde K_t^{cmp})\n",
    "$$\n",
    "\n",
    "- $q_t \\in(d_k, 1)$；$\\tilde K_t^{cmp}\\in (d_k, M)$\n",
    "- $p_t^{cmp}=(1, M)$\n",
    "- $\\ell'$：也是 (selection) block size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef843e6-2402-4b19-b1b2-38b9cac4b458",
   "metadata": {},
   "source": [
    "## kernel 优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6ecd1-bdc4-4919-a0e6-b63db57dbfa0",
   "metadata": {},
   "source": [
    "- 常规：我们需要将单头KV，复制成与Q头数相对应的KV，送到SRAM计算注意力\n",
    "- NSA：单头KV送到SRAM，这里不需要复制多头。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
