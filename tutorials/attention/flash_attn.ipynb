{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741bbab9-b105-45d4-8fdf-751eda316d54",
   "metadata": {},
   "source": [
    "- Transformer 是现代人工智能的核心，而attention是 Transformer 最具特色的机制，在 transformer 及 attention 上花再多的时间探索都是值得。\n",
    "- 从加速计算/存储的角度优化 attention 计算，都是在 attention 的计算机制上做文章：\n",
    "    - vllm 中的 paged attention：推理优化\n",
    "    - 以及今天要讲的 flash attention（考虑到硬件的读取和计算）：不只是推理；\n",
    "        - sdpa：`torch.nn.functional.scaled_dot_product_attention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92401c27-7dc7-4774-a3e8-63e35aa16bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:20:54.278071Z",
     "iopub.status.busy": "2024-07-06T04:20:54.277452Z",
     "iopub.status.idle": "2024-07-06T04:20:55.639969Z",
     "shell.execute_reply": "2024-07-06T04:20:55.638599Z",
     "shell.execute_reply.started": "2024-07-06T04:20:54.278027Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2734e0-2359-4e17-a6b8-03e2499041f7",
   "metadata": {},
   "source": [
    "\n",
    "https://github.com/Dao-AILab/flash-attention\n",
    "\n",
    "- install\n",
    "    \n",
    "    ```\n",
    "    # pip\n",
    "    pip install flash-attn --no-build-isolation\n",
    "    pip install flash_attn -U --force-reinstall\n",
    "    \n",
    "    # source code compile\n",
    "    python setup.py install\n",
    "    ```\n",
    "- references\n",
    "    - paper\n",
    "        - [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)\n",
    "        - [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://tridao.me/publications/flash2/flash2.pdf)\n",
    "    - https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad\n",
    "    - https://medium.com/@e0928021388/%E7%AA%81%E7%A0%B4-transformers-%E7%9A%84%E9%80%9F%E5%BA%A6%E7%93%B6%E9%A0%B8-flash-attention-%E4%BB%8B%E7%B4%B9-28c1bc667fd9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc577fd6-664a-4100-a933-77eda1da6896",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55191ac5-073f-4cf1-8423-ab03e4951b1e",
   "metadata": {},
   "source": [
    "> 算法（软件）、硬件协同优化；\n",
    "\n",
    "- Memory\n",
    "    - SRAM > HBM > DRAM\n",
    "    - SRAM：Static RAM（Random Access Memory）\n",
    "        - 每个 SM（Stream multiproecssors，流多处理器）192KB （A100 108个，4090 128个）\n",
    "            - 108*192/1024 = 20MB\n",
    "    - HBM：high bandwidth memory（4090 24GB，A100 80GB）\n",
    "- GPU 读写&计算（compute-bound vs. memory-bound)\n",
    "    - operations fused：将好几个 operations fuse 成一个 operation 进而减轻 memory 存取的 loading\n",
    "    - 在GPU当中有非常大量的 threads （kernel） 负责执行 operation 的运算，而整个运算的过程基本上是从 HBM 当中将资料加载至 SRAM 中，执行运算并将 output 存回 HBM 当中。\n",
    "    - compute-bound\n",
    "        - 运算的主要时间都耗费在 operation 的计算上，HBM 的存取只占了其中一点点的时间\n",
    "        - 像是多维度的矩阵相乘或是高 channel 数的 convolution 都属于这类。\n",
    "    - memory-bound\n",
    "        - 主要时间都耗费在 memory 的读取上，而实际的运算只占了其中一点点的时间\n",
    "        - elementwise （e.g.， activation， dropout） and **reduction** （e.g.， sum， softmax， batch norm， layer norm）\n",
    "- attention QKV 计算\n",
    "    - 分块矩阵，然后是 loop（outer loop，inner loop，对应的是 gpu cuda 的 kernel 优化）；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d407c3-0981-4149-a3db-f3ca3cf42487",
   "metadata": {},
   "source": [
    "### R/W and operations fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79fed942-918a-4524-aa36-e8ceb458d833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T07:57:07.647997Z",
     "iopub.status.busy": "2024-07-06T07:57:07.647408Z",
     "iopub.status.idle": "2024-07-06T07:57:07.660867Z",
     "shell.execute_reply": "2024-07-06T07:57:07.658789Z",
     "shell.execute_reply.started": "2024-07-06T07:57:07.647951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/sram_comp.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/sram_comp.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae159ebe-4fa6-4d33-977b-e8d06546b6c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T07:44:27.046576Z",
     "iopub.status.busy": "2024-07-06T07:44:27.045982Z",
     "iopub.status.idle": "2024-07-06T07:44:27.058825Z",
     "shell.execute_reply": "2024-07-06T07:44:27.056647Z",
     "shell.execute_reply.started": "2024-07-06T07:44:27.046531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/0*0Yn1aLye8s6_WTOu.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:828/format:webp/0*0Yn1aLye8s6_WTOu.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7342e2-a9fa-4d30-9545-07701fd35844",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "- A **kernel** is basically a fancy way of saying “a GPU operation”.\n",
    "- **Fusion** means you’re fusing/combining multiple ops together.\n",
    "\n",
    "\n",
    "```\n",
    "# 独立的内核调用\n",
    "a = x + y  # 内核1\n",
    "b = a * z  # 内核2\n",
    "c = torch.relu(b)  # 内核3\n",
    "\n",
    "# 优化后的内核（操作融合为一个内核）\n",
    "# 定义操作融合的内核（使用 TorchScript）\n",
    "@torch.jit.script\n",
    "def fused_kernel(x, y, z):\n",
    "    a = x + y\n",
    "    b = a * z\n",
    "    c = torch.relu(b)\n",
    "    return c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650308a7-3689-451b-ba3d-d5bcaf20a23f",
   "metadata": {},
   "source": [
    "### review of Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb366f54-4d01-47b2-a68c-951a6efbd83f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T03:38:04.039967Z",
     "iopub.status.busy": "2024-07-06T03:38:04.039328Z",
     "iopub.status.idle": "2024-07-06T03:38:04.052132Z",
     "shell.execute_reply": "2024-07-06T03:38:04.049890Z",
     "shell.execute_reply.started": "2024-07-06T03:38:04.039922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/attention_steps.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/attention_steps.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139127ee-074a-4af5-8889-ba6d23aba1f4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{O} = \\text{Dropout}(\\text{Softmax}(\\text{Mask}(\\mathbf{QK}^T)))\\mathbf{V}\n",
    "$$\n",
    "\n",
    "- formula\n",
    "    - $(\\mathbf {Q, K, V})\\in\\mathbb R^{N\\times d}$\n",
    "    - $\\mathbf{QK}^T\\in\\mathbb R^{N\\times N}$\n",
    "    - $\\mathbf A=\\text{Dropout}(\\text{Softmax}(\\text{Mask}(\\mathbf{QK}^T)))$\n",
    "        - mask: m, softmax: sm, dropout: do\n",
    "    - $\\mathbf O=\\mathbf {AV}\\in \\mathbb R^{N\\times d}$\n",
    "- notes\n",
    "    - attention is bottlenecked by memory reads/writes\n",
    "    - naive implementation requires repeated R/W from slow GPU HBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "90903ce4-4890-478c-acde-09cf9e6f113f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T08:31:45.053435Z",
     "iopub.status.busy": "2024-07-06T08:31:45.052804Z",
     "iopub.status.idle": "2024-07-06T08:31:45.066064Z",
     "shell.execute_reply": "2024-07-06T08:31:45.063948Z",
     "shell.execute_reply.started": "2024-07-06T08:31:45.053389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/stand_attn.png\" width=\"800\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/stand_attn.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7e3a7-2cf7-44ac-85c4-9d4fcaf5aa15",
   "metadata": {},
   "source": [
    "-  memory access 的时间复杂度为 $O（N*D + N*N）$，其中通常 N >> D（e.g.， N 为 4096 而 d 为 64），因此我们可以发现 S 和 P 的 memory access （$N*N$ 的复杂度） 便是整体 self-attention 的 bottleneck！\n",
    "- 我们可以发现对于整个 self-attention 当中，其实我们真正需要的是最后面的 output O 而已，过程当中不管 P 和 S 长什么样子其实对于我们来说都没有很重要，既然他不重要为什么我们还是要将他存入 HBM 呢？ 主要是因为以下两个理由：\n",
    "    - 我们需要这些 intermediate activations 来帮助我们在 backward 的时候通过 backpropagation 计算 gradients，这也使得我们很难将多个 operations fuse 成一个 operation。\n",
    "    - 由于 SRAM 本身不够大，而 softmax 这种需要计算 sum 的 operation，需要整个 row 的 element 都到齐后才可以计算，使得我们沒有办法 apply 一些 divide and conquery 的 algorithm ，更使得我们没有办法把所有运算一口气在 SRAM当中计算完。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548103e-23f6-4295-89c7-c30651badc83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7171a317-bbb4-4d0f-b4d1-59fdbdaa327b",
   "metadata": {},
   "source": [
    "## flash attention (Tiling & Recomputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e98b3b0c-e505-4a05-9351-c40dcb283fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T03:24:53.925775Z",
     "iopub.status.busy": "2024-07-06T03:24:53.925201Z",
     "iopub.status.idle": "2024-07-06T03:24:53.938085Z",
     "shell.execute_reply": "2024-07-06T03:24:53.936036Z",
     "shell.execute_reply.started": "2024-07-06T03:24:53.925731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/flash-attn.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/flash-attn.png', width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "15f3b9b7-f683-47ba-944e-63d94d4292ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T07:40:07.995573Z",
     "iopub.status.busy": "2024-07-06T07:40:07.994992Z",
     "iopub.status.idle": "2024-07-06T07:40:08.007151Z",
     "shell.execute_reply": "2024-07-06T07:40:08.005108Z",
     "shell.execute_reply.started": "2024-07-06T07:40:07.995531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/stand_attn.png\" width=\"800\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/stand_attn.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da9fbe-123a-45bb-849f-6e40f44e7363",
   "metadata": {},
   "source": [
    "### tiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb5efa-4785-4961-8bb8-917e75b9cba7",
   "metadata": {},
   "source": [
    "- 这是一个现实生活的概念（谷歌搜图）\n",
    "- 前向后向都会用得到\n",
    "- chunking the NxN softmax/scores matrix into blocks.\n",
    "- `(Q, K) => S => P => O` => `(Q, K) => O`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb47be-8694-4106-88b3-ef0462ddd119",
   "metadata": {},
   "source": [
    "- softmax of vector $x\\in \\mathbb R^B$（element-wise）\n",
    "\n",
    "    $$\n",
    "    m(x):=\\max_i x_i, \\quad f(x):=[e^{x_1-m(x)}, \\cdots, e^{x_B-m(x)}], \\quad \\ell(x)=\\sum f(x), \\quad \\text{softmax}(x):=\\frac{f(x)}{\\ell(x)}\n",
    "    $$\n",
    "  - 减去 max value 来避免经过 exponential 后 overflow\n",
    "\n",
    "- vectors $x^{(1)},x^{(2)}\\in \\mathbb R^B, x=[x^{(1)}\\quad x^{(2)}]\\in \\mathbb R^{2B}$（element-wise）\n",
    "\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    m(x) &= m\\left( \\left[ x^{(1)}, x^{(2)} \\right] \\right) = \\max \\left( m(x^{(1)}), m(x^{(2)}) \\right), \\\\\n",
    "    f(x) &= \\left[ e^{m(x^{(1)}) - m(x)} f(x^{(1)}), \\, e^{m(x^{(2)}) - m(x)} f(x^{(2)}) \\right], \\\\\n",
    "    \\ell(x) &= \\ell\\left( \\left[ x^{(1)}, x^{(2)} \\right] \\right) = e^{m(x^{(1)}) - m(x)} \\ell(x^{(1)}) + e^{m(x^{(2)}) - m(x)} \\ell(x^{(2)}), \\\\\n",
    "    \\text{softmax}(x) &= \\frac{f(x)}{\\ell(x)}.\n",
    "    \\end{split}\n",
    "    $$\n",
    "  - 这里的 $f(x^{(1)})=[e^{x_1-m(x^{(1)})}, \\cdots]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0788a82a-3365-4250-9f3e-31ddbf0c6a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:11:33.610765Z",
     "iopub.status.busy": "2024-07-06T05:11:33.610138Z",
     "iopub.status.idle": "2024-07-06T05:11:33.625403Z",
     "shell.execute_reply": "2024-07-06T05:11:33.623156Z",
     "shell.execute_reply.started": "2024-07-06T05:11:33.610721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "caa90494-4bef-455d-8884-fab54debbb12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:09:54.516810Z",
     "iopub.status.busy": "2024-07-06T05:09:54.516238Z",
     "iopub.status.idle": "2024-07-06T05:09:54.529987Z",
     "shell.execute_reply": "2024-07-06T05:09:54.527974Z",
     "shell.execute_reply.started": "2024-07-06T05:09:54.516768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0871, 0.2369, 0.6439])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "991f31a9-b9ec-4356-91a6-a7dc87fcf5ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:09:55.814641Z",
     "iopub.status.busy": "2024-07-06T05:09:55.814040Z",
     "iopub.status.idle": "2024-07-06T05:09:55.829311Z",
     "shell.execute_reply": "2024-07-06T05:09:55.827191Z",
     "shell.execute_reply.started": "2024-07-06T05:09:55.814598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0871, 0.2369, 0.6439])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.max(x)\n",
    "f = torch.exp(x - m)\n",
    "l = torch.sum(f)\n",
    "f / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ed2f481-7dc1-407f-9915-ba886d681548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:17:14.321734Z",
     "iopub.status.busy": "2024-07-06T05:17:14.321122Z",
     "iopub.status.idle": "2024-07-06T05:17:14.341514Z",
     "shell.execute_reply": "2024-07-06T05:17:14.339401Z",
     "shell.execute_reply.started": "2024-07-06T05:17:14.321690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0871, 0.2369, 0.6439])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = x[:2]\n",
    "x_2 = x[2:]\n",
    "m = torch.max(x)\n",
    "m_1 = torch.max(x_1)\n",
    "m_2 = torch.max(x_2)\n",
    "\n",
    "f_1 = torch.exp(x_1 - m_1)\n",
    "f_2 = torch.exp(x_2 - m_2)\n",
    "\n",
    "l_1 = torch.sum(f_1)\n",
    "l_2 = torch.sum(f_2)\n",
    "\n",
    "f = torch.cat((torch.exp(m_1 - m) * f_1, torch.exp(m_2 - m) * f_2))\n",
    "l = torch.exp(m_1 - m) * l_1 + torch.exp(m_2 - m) * l_2\n",
    "f/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "420fd362-63dd-4212-9fd1-4879a0b30dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:57:30.085782Z",
     "iopub.status.busy": "2024-07-06T04:57:30.085168Z",
     "iopub.status.idle": "2024-07-06T04:57:30.098331Z",
     "shell.execute_reply": "2024-07-06T04:57:30.096117Z",
     "shell.execute_reply.started": "2024-07-06T04:57:30.085738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/flash-attn-algo.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/flash-attn-algo.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47019c-4863-4e91-9395-4a4fd0bf585c",
   "metadata": {},
   "source": [
    "- $\\mathbf {Q,K,V,O}$ 分别做行分块（row blocks），$\\mathbf O$ 是结果矩阵 => $4d$\n",
    "    - $\\mathbf Q_i,\\mathbf O_i\\in \\mathbb R^{B_r\\times d}$\n",
    "    - $\\mathbf {K}_j,\\mathbf {V}_j\\in\\mathbb R^{B_c\\times d}$\n",
    "- $\\ell$：row exp sum，$m$：row max\n",
    "- 对 $T_c$（$\\mathbf K_j,\\mathbf V_j$） 做外循环，对 $T_r$（$\\mathbf Q_i, \\mathbf O_i$）做内循环\n",
    "    - 内循环不断地 update $\\mathbf O_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1fa43f2-d6c1-48ac-9449-6d5fe01556cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:21:43.221740Z",
     "iopub.status.busy": "2024-07-06T05:21:43.221114Z",
     "iopub.status.idle": "2024-07-06T05:21:43.232343Z",
     "shell.execute_reply": "2024-07-06T05:21:43.230179Z",
     "shell.execute_reply.started": "2024-07-06T05:21:43.221697Z"
    }
   },
   "outputs": [],
   "source": [
    "s = torch.tensor([0.1, 0.3, 0.5, 0.7])\n",
    "v = torch.tensor([7, 8,  9, 10], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47713072-99c7-440a-ac41-9c58ee81f594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:21:44.641715Z",
     "iopub.status.busy": "2024-07-06T05:21:44.641132Z",
     "iopub.status.idle": "2024-07-06T05:21:44.654538Z",
     "shell.execute_reply": "2024-07-06T05:21:44.652483Z",
     "shell.execute_reply.started": "2024-07-06T05:21:44.641673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1807, 0.2207, 0.2695, 0.3292])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.softmax(s, dim=-1)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27b1e523-0b0f-499c-80c3-e97512bf67df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:21:47.946302Z",
     "iopub.status.busy": "2024-07-06T05:21:47.945728Z",
     "iopub.status.idle": "2024-07-06T05:21:47.958799Z",
     "shell.execute_reply": "2024-07-06T05:21:47.956790Z",
     "shell.execute_reply.started": "2024-07-06T05:21:47.946260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7472)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8540470-f719-4023-9913-231a98139992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:23:31.035856Z",
     "iopub.status.busy": "2024-07-06T05:23:31.035150Z",
     "iopub.status.idle": "2024-07-06T05:23:31.046131Z",
     "shell.execute_reply": "2024-07-06T05:23:31.043956Z",
     "shell.execute_reply.started": "2024-07-06T05:23:31.035738Z"
    }
   },
   "outputs": [],
   "source": [
    "# tiling, sub-blocks\n",
    "s_1, s_2 = s[:2], s[2:]\n",
    "v_1, v_2 = v[:2], v[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9308b984-4c78-44a6-b5a0-b29870c0112c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T07:08:47.495915Z",
     "iopub.status.busy": "2024-07-06T07:08:47.495211Z",
     "iopub.status.idle": "2024-07-06T07:08:47.512275Z",
     "shell.execute_reply": "2024-07-06T07:08:47.510190Z",
     "shell.execute_reply.started": "2024-07-06T07:08:47.495866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.5498), tensor(2.4550))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1\n",
    "p_1 = torch.softmax(s_1, dim=-1)\n",
    "exp_sum_1 = torch.sum(torch.exp(s_1))\n",
    "# 需要额外存储 exp_sum_1\n",
    "p_1 @ v_1, exp_sum_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2ed577e1-579e-4152-a413-b11f15dcb880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:26:24.950271Z",
     "iopub.status.busy": "2024-07-06T05:26:24.948958Z",
     "iopub.status.idle": "2024-07-06T05:26:24.964419Z",
     "shell.execute_reply": "2024-07-06T05:26:24.962343Z",
     "shell.execute_reply.started": "2024-07-06T05:26:24.950226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5498), tensor(3.6625))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2\n",
    "p_2 = torch.softmax(s_2, dim=-1)\n",
    "exp_sum_2 = torch.sum(torch.exp(s_2))\n",
    "p_2 @ v_2, exp_sum_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "199ec0a2-b0f1-4467-80b3-b56d34d652ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T07:24:22.614891Z",
     "iopub.status.busy": "2024-07-06T07:24:22.614264Z",
     "iopub.status.idle": "2024-07-06T07:24:22.626850Z",
     "shell.execute_reply": "2024-07-06T07:24:22.624723Z",
     "shell.execute_reply.started": "2024-07-06T07:24:22.614847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.74685641864268"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3\n",
    "# 这里可以看到 sub-blocks 对应的计算可以独立进行（不用 care S&P 的具体形式），只需要额外存储 exp_sum\n",
    "(9.5498 * 3.66 + 7.5498 * 2.455) / (2.455 + 3.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "958a4276-0865-4290-80b5-1151b293d3de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T07:27:14.237650Z",
     "iopub.status.busy": "2024-07-06T07:27:14.237016Z",
     "iopub.status.idle": "2024-07-06T07:27:14.249985Z",
     "shell.execute_reply": "2024-07-06T07:27:14.247846Z",
     "shell.execute_reply.started": "2024-07-06T07:27:14.237605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-MeAwCRNds5prU9HiSmuQ.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-MeAwCRNds5prU9HiSmuQ.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9874a20-458b-4bbf-84a8-1f2860884225",
   "metadata": {},
   "source": [
    "- $QKV$ 的计算发生在 SRAM 上；\n",
    "- 尽管这样的方式不能让我们避免`O（N*N）` 的时间复杂度（因为我们需要 for loop 将每个 Key vector 和 Query vector 做内积，上图所示），但是这样切割成 sub-block 直接计算出结果，且不用整个 row 一起存取的方式，整个时间复杂度除以 M （sub-block 数量），同时减少许多 O（N*N） memory 存取的次数，还是可以达到非常显著的效果提升！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b31bbf-57a0-420f-a679-0eb0688423bc",
   "metadata": {},
   "source": [
    "### Recomputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d5d9e-1c94-4e0f-b839-36a474d814fb",
   "metadata": {},
   "source": [
    "> 不储存 intermediate activations (attn matrix, $S,P$) 而是在有需要的时候（比如 backward）再重新计算\n",
    "> \n",
    "- backward only\n",
    "- 概念其实类似于 gradient checkpointing（也是一个再计算的逻辑），然而 gradient checkpointing 的主要精神是稍微牺牲一些速度但是可以大幅度的减少 GPU memory 的需求 （时间换取空间的感觉），而在这边 flash attention 当中的 recomputation 这样的做法除了可以节省 GPU memory 之外还可以加速！\n",
    "- 当我们在计算backward时，我们本来就要将 K， Q， and V 加载 SRAM，而与其我们在 forward 时将 S 和 P 这两个 N*N 的 matrix 存入 HBM， 并且在 backward 时再将他们两个从 HBM load 到 SRAM，我们直接用本来就在 SRAM 当中的 K， Q， and V 重新计算出 S 和 P 反而可以更快。 这点也反应了 HBM 本身相较于 SRAM 和 GPU computing 速度的差距！\n",
    "- 結合了 Tiling 和 Recomputation，使得 flash attention 有办法 operations fuse 成一個 operation，更進一步避免了 HBM 的 read 和 write 的 loading，而不用担心 fusion 后使得在 backward 时会无法进行 chain rule。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
