{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741bbab9-b105-45d4-8fdf-751eda316d54",
   "metadata": {},
   "source": [
    "- Transformer 是现代人工智能的核心，而attention是 Transformer 最具特色的机制，在 transformer 及 attention 上花再多的时间探索都是值得。\n",
    "- 从加速计算/存储的角度优化 attention 计算，都是在 attention 的计算机制上做文章：\n",
    "    - vllm 中的 paged attention；\n",
    "    - 以及今天要讲的 flash attention（考虑到硬件的读取和计算）；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92401c27-7dc7-4774-a3e8-63e35aa16bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:20:54.278071Z",
     "iopub.status.busy": "2024-07-06T04:20:54.277452Z",
     "iopub.status.idle": "2024-07-06T04:20:55.639969Z",
     "shell.execute_reply": "2024-07-06T04:20:55.638599Z",
     "shell.execute_reply.started": "2024-07-06T04:20:54.278027Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2734e0-2359-4e17-a6b8-03e2499041f7",
   "metadata": {},
   "source": [
    "\n",
    "https://github.com/Dao-AILab/flash-attention\n",
    "\n",
    "- install\n",
    "    \n",
    "    ```\n",
    "    # pip\n",
    "    pip install flash-attn --no-build-isolation\n",
    "    pip install flash_attn -U --force-reinstall\n",
    "    \n",
    "    # source code compile\n",
    "    python setup.py install\n",
    "    ```\n",
    "- references\n",
    "    - paper\n",
    "        - [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)\n",
    "        - [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://tridao.me/publications/flash2/flash2.pdf)\n",
    "    - https://medium.com/@e0928021388/%E7%AA%81%E7%A0%B4-transformers-%E7%9A%84%E9%80%9F%E5%BA%A6%E7%93%B6%E9%A0%B8-flash-attention-%E4%BB%8B%E7%B4%B9-28c1bc667fd9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc577fd6-664a-4100-a933-77eda1da6896",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55191ac5-073f-4cf1-8423-ab03e4951b1e",
   "metadata": {},
   "source": [
    "> 算法（软件）、硬件协同优化；\n",
    "\n",
    "- Memory\n",
    "    - SRAM > HBM > DRAM\n",
    "    - SRAM：Static RAM（Random Access Memory）\n",
    "        - 每个 SM（Stream multiproecssors，流多处理器）192KB （A100 108个，4090 128个）\n",
    "            - 108*192/1024 = 20MB\n",
    "    - HBM：high bandwidth memory，4090:24GB\n",
    "- GPU 读写&计算（compute-bound vs. memory-bound)\n",
    "    - operations fused：将好几个 operations fuse 成一个 operation 进而减轻 memory 存取的 loading\n",
    "    - 在GPU当中有非常大量的 threads （kernel） 负责执行 operation 的运算，而整个运算的过程基本上是从 HBM 当中将资料加载至 SRAM 中，执行运算并将 output 存回 HBM 当中。\n",
    "    - compute-bound\n",
    "        - 运算的主要时间都耗费在 operation 的计算上，HBM 的存取只占了其中一点点的时间\n",
    "        - 像是多维度的矩阵相乘或是高 channel 数的 convolution 都属于这类。\n",
    "    - memory-bound\n",
    "        - 主要时间都耗费在 memory 的读取上，而实际的运算只占了其中一点点的时间\n",
    "        - elementwise （e.g.， activation， dropout） and reduction （e.g.， sum， softmax， batch norm， layer norm）\n",
    "- attention QKV 计算\n",
    "    - 分块矩阵，然后是 loop（outer loop，inner loop，对应的是 gpu cuda 的 kernel 优化）；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650308a7-3689-451b-ba3d-d5bcaf20a23f",
   "metadata": {},
   "source": [
    "### review of Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb366f54-4d01-47b2-a68c-951a6efbd83f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T03:38:04.039967Z",
     "iopub.status.busy": "2024-07-06T03:38:04.039328Z",
     "iopub.status.idle": "2024-07-06T03:38:04.052132Z",
     "shell.execute_reply": "2024-07-06T03:38:04.049890Z",
     "shell.execute_reply.started": "2024-07-06T03:38:04.039922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/attention_steps.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/attention_steps.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139127ee-074a-4af5-8889-ba6d23aba1f4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{O} = \\text{Dropout}(\\text{Softmax}(\\text{Mask}(\\mathbf{QK}^T)))\\mathbf{V}\n",
    "$$\n",
    "\n",
    "- formula\n",
    "    - $(\\mathbf {Q, K, V})\\in\\mathbb R^{N\\times d}$\n",
    "    - $\\mathbf{QK}^T\\in\\mathbb R^{N\\times N}$\n",
    "    - $\\mathbf A=\\text{Dropout}(\\text{Softmax}(\\text{Mask}(\\mathbf{QK}^T)))$\n",
    "        - mask: m, softmax: sm, dropout: do\n",
    "    - $\\mathbf O=\\mathbf {AV}\\in \\mathbb R^{N\\times N}$\n",
    "- notes\n",
    "    - attention is bottlenecked by memory reads/writes\n",
    "    - naive implementation requires repeated R/W from slow GPU HBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90903ce4-4890-478c-acde-09cf9e6f113f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:11:15.140438Z",
     "iopub.status.busy": "2024-07-06T04:11:15.139823Z",
     "iopub.status.idle": "2024-07-06T04:11:15.153216Z",
     "shell.execute_reply": "2024-07-06T04:11:15.151119Z",
     "shell.execute_reply.started": "2024-07-06T04:11:15.140393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/stand_attn.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/stand_attn.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7e3a7-2cf7-44ac-85c4-9d4fcaf5aa15",
   "metadata": {},
   "source": [
    "-  memory access 的时间复杂度为 $O（N*D + N*N）$，其中通常 N >> D（e.g.， N 为 4096 而 d 为 64），因此我们可以发现 S 和 P 的 memory access （$N*N$ 的复杂度） 便是整体 self-attention 的 bottleneck！\n",
    "- 我们可以发现对于整个 self-attention 当中，其实我们真正需要的是最后面的 output O 而已，过程当中不管 P 和 S 长什么样子其实对于我们来说都没有很重要，既然他不重要为什么我们还是要将他存入 HBM 呢？ 主要是因为以下两个理由：\n",
    "    - 我们需要这些 intermediate activations 来帮助我们在 backward 的时候通过 backpropagation 计算 gradients，这也使得我们很难将多个 operations fuse 成一个 operation。\n",
    "    - 由于 SRAM 本身不够大，而 softmax 这种需要计算 sum 的 operation，需要整个 row 的 element 都到齐后才可以计算，使得我们沒有办法 apply 一些 divide and conquery 的 algorithm ，更使得我们没有办法把所有运算一口气在 SRAM当中计算完。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548103e-23f6-4295-89c7-c30651badc83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7171a317-bbb4-4d0f-b4d1-59fdbdaa327b",
   "metadata": {},
   "source": [
    "## flash attention (Tiling & Recomputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e98b3b0c-e505-4a05-9351-c40dcb283fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T03:24:53.925775Z",
     "iopub.status.busy": "2024-07-06T03:24:53.925201Z",
     "iopub.status.idle": "2024-07-06T03:24:53.938085Z",
     "shell.execute_reply": "2024-07-06T03:24:53.936036Z",
     "shell.execute_reply.started": "2024-07-06T03:24:53.925731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/flash-attn.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/flash-attn.png', width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15f3b9b7-f683-47ba-944e-63d94d4292ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:16:52.251550Z",
     "iopub.status.busy": "2024-07-06T04:16:52.250902Z",
     "iopub.status.idle": "2024-07-06T04:16:52.264548Z",
     "shell.execute_reply": "2024-07-06T04:16:52.262454Z",
     "shell.execute_reply.started": "2024-07-06T04:16:52.251504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/stand_attn.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/stand_attn.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da9fbe-123a-45bb-849f-6e40f44e7363",
   "metadata": {},
   "source": [
    "### tiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb5efa-4785-4961-8bb8-917e75b9cba7",
   "metadata": {},
   "source": [
    "- `(Q, K) => S => P => O` => `(Q, K) => O`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb47be-8694-4106-88b3-ef0462ddd119",
   "metadata": {},
   "source": [
    "- softmax of vector $x\\in \\mathbb R^B$（element-wise）\n",
    "\n",
    "    $$\n",
    "    m(x):=\\max_i x_i, \\quad f(x):=[e^{x_1-m(x)}, \\cdots, e^{x_B-m(x)}], \\quad \\ell(x)=\\sum f(x), \\quad \\text{softmax}(x):=\\frac{f(x)}{\\ell(x)}\n",
    "    $$\n",
    "\n",
    "- vectors $x^{(1)},x^{(2)}\\in \\mathbb R^B, x=[x^{(1)}\\quad x^{(2)}]\\in \\mathbb R^{2B}$（element-wise）\n",
    "\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    m(x) &= m\\left( \\left[ x^{(1)}, x^{(2)} \\right] \\right) = \\max \\left( m(x^{(1)}), m(x^{(2)}) \\right), \\\\\n",
    "    f(x) &= \\left[ e^{m(x^{(1)}) - m(x)} f(x^{(1)}), \\, e^{m(x^{(2)}) - m(x)} f(x^{(2)}) \\right], \\\\\n",
    "    \\ell(x) &= \\ell\\left( \\left[ x^{(1)}, x^{(2)} \\right] \\right) = e^{m(x^{(1)}) - m(x)} \\ell(x^{(1)}) + e^{m(x^{(2)}) - m(x)} \\ell(x^{(2)}), \\\\\n",
    "    \\text{softmax}(x) &= \\frac{f(x)}{\\ell(x)}.\n",
    "    \\end{split}\n",
    "    $$\n",
    "  - 这里的 $f(x^{(1)})=[e^{x_1-m(x^{(1)})}, \\cdots]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0788a82a-3365-4250-9f3e-31ddbf0c6a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:11:33.610765Z",
     "iopub.status.busy": "2024-07-06T05:11:33.610138Z",
     "iopub.status.idle": "2024-07-06T05:11:33.625403Z",
     "shell.execute_reply": "2024-07-06T05:11:33.623156Z",
     "shell.execute_reply.started": "2024-07-06T05:11:33.610721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "caa90494-4bef-455d-8884-fab54debbb12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:09:54.516810Z",
     "iopub.status.busy": "2024-07-06T05:09:54.516238Z",
     "iopub.status.idle": "2024-07-06T05:09:54.529987Z",
     "shell.execute_reply": "2024-07-06T05:09:54.527974Z",
     "shell.execute_reply.started": "2024-07-06T05:09:54.516768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0871, 0.2369, 0.6439])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "991f31a9-b9ec-4356-91a6-a7dc87fcf5ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:09:55.814641Z",
     "iopub.status.busy": "2024-07-06T05:09:55.814040Z",
     "iopub.status.idle": "2024-07-06T05:09:55.829311Z",
     "shell.execute_reply": "2024-07-06T05:09:55.827191Z",
     "shell.execute_reply.started": "2024-07-06T05:09:55.814598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0871, 0.2369, 0.6439])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.max(x)\n",
    "f = torch.exp(x - m)\n",
    "l = torch.sum(f)\n",
    "f / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ed2f481-7dc1-407f-9915-ba886d681548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T05:17:14.321734Z",
     "iopub.status.busy": "2024-07-06T05:17:14.321122Z",
     "iopub.status.idle": "2024-07-06T05:17:14.341514Z",
     "shell.execute_reply": "2024-07-06T05:17:14.339401Z",
     "shell.execute_reply.started": "2024-07-06T05:17:14.321690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0871, 0.2369, 0.6439])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = x[:2]\n",
    "x_2 = x[2:]\n",
    "m = torch.max(x)\n",
    "m_1 = torch.max(x_1)\n",
    "m_2 = torch.max(x_2)\n",
    "\n",
    "f_1 = torch.exp(x_1 - m_1)\n",
    "f_2 = torch.exp(x_2 - m_2)\n",
    "\n",
    "l_1 = torch.sum(f_1)\n",
    "l_2 = torch.sum(f_2)\n",
    "\n",
    "f = torch.cat((torch.exp(m_1 - m) * f_1, torch.exp(m_2 - m) * f_2))\n",
    "l = torch.exp(m_1 - m) * l_1 + torch.exp(m_2 - m) * l_2\n",
    "f/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "420fd362-63dd-4212-9fd1-4879a0b30dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:57:30.085782Z",
     "iopub.status.busy": "2024-07-06T04:57:30.085168Z",
     "iopub.status.idle": "2024-07-06T04:57:30.098331Z",
     "shell.execute_reply": "2024-07-06T04:57:30.096117Z",
     "shell.execute_reply.started": "2024-07-06T04:57:30.085738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/flash-attn-algo.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/flash-attn-algo.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47019c-4863-4e91-9395-4a4fd0bf585c",
   "metadata": {},
   "source": [
    "- $\\mathbf {Q,K,V,O}$ 分别做行分块（row blocks），$\\mathbf O$ 是结果矩阵\n",
    "    - $\\mathbf Q_i,\\mathbf O_i\\in \\mathbb R^{B_r\\times d}$\n",
    "    - $\\mathbf {K}_j,\\mathbf {V}_j\\in\\mathbb R^{B_c\\times d}$\n",
    "- 对 $T_c$（$\\mathbf K_j,\\mathbf V_j$） 做外循环，对 $T_r$（$\\mathbf Q_i, \\mathbf O_i$）做内循环\n",
    "    - 内循环不断地 update $\\mathbf O_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1fa43f2-d6c1-48ac-9449-6d5fe01556cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:32:59.342392Z",
     "iopub.status.busy": "2024-07-06T04:32:59.341803Z",
     "iopub.status.idle": "2024-07-06T04:32:59.351555Z",
     "shell.execute_reply": "2024-07-06T04:32:59.349678Z",
     "shell.execute_reply.started": "2024-07-06T04:32:59.342349Z"
    }
   },
   "outputs": [],
   "source": [
    "s = torch.tensor([0.1, 0.3, 0.5, 0.7])\n",
    "v = torch.tensor([7, 8,  9, 10], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47713072-99c7-440a-ac41-9c58ee81f594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:22:53.683548Z",
     "iopub.status.busy": "2024-07-06T04:22:53.682934Z",
     "iopub.status.idle": "2024-07-06T04:22:53.697441Z",
     "shell.execute_reply": "2024-07-06T04:22:53.695390Z",
     "shell.execute_reply.started": "2024-07-06T04:22:53.683506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1807, 0.2207, 0.2695, 0.3292])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.softmax(s, dim=-1)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27b1e523-0b0f-499c-80c3-e97512bf67df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:23:01.658801Z",
     "iopub.status.busy": "2024-07-06T04:23:01.658153Z",
     "iopub.status.idle": "2024-07-06T04:23:01.679436Z",
     "shell.execute_reply": "2024-07-06T04:23:01.676844Z",
     "shell.execute_reply.started": "2024-07-06T04:23:01.658757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7472)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8540470-f719-4023-9913-231a98139992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:29:53.787573Z",
     "iopub.status.busy": "2024-07-06T04:29:53.786935Z",
     "iopub.status.idle": "2024-07-06T04:29:53.801381Z",
     "shell.execute_reply": "2024-07-06T04:29:53.799234Z",
     "shell.execute_reply.started": "2024-07-06T04:29:53.787529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.3000],\n",
       "        [0.5000, 0.7000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tiling\n",
    "s = s.view(2, 2)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9308b984-4c78-44a6-b5a0-b29870c0112c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:30:25.608939Z",
     "iopub.status.busy": "2024-07-06T04:30:25.608336Z",
     "iopub.status.idle": "2024-07-06T04:30:25.619989Z",
     "shell.execute_reply": "2024-07-06T04:30:25.618565Z",
     "shell.execute_reply.started": "2024-07-06T04:30:25.608894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4502, 0.5498],\n",
       "        [0.4502, 0.5498]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.softmax(s, dim=-1)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ed577e1-579e-4152-a413-b11f15dcb880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:33:01.503966Z",
     "iopub.status.busy": "2024-07-06T04:33:01.503345Z",
     "iopub.status.idle": "2024-07-06T04:33:01.517805Z",
     "shell.execute_reply": "2024-07-06T04:33:01.515847Z",
     "shell.execute_reply.started": "2024-07-06T04:33:01.503920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  9.],\n",
       "        [ 8., 10.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.transpose(v.view(2, 2), 0, 1)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2731b12e-8fda-4cb8-9120-a1091d30c05b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:33:04.437822Z",
     "iopub.status.busy": "2024-07-06T04:33:04.437240Z",
     "iopub.status.idle": "2024-07-06T04:33:04.451225Z",
     "shell.execute_reply": "2024-07-06T04:33:04.449204Z",
     "shell.execute_reply.started": "2024-07-06T04:33:04.437779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.5498)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0, :] @ v[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e19a9de7-ee2f-4bb5-bb84-71456b935c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:33:06.094067Z",
     "iopub.status.busy": "2024-07-06T04:33:06.093504Z",
     "iopub.status.idle": "2024-07-06T04:33:06.107072Z",
     "shell.execute_reply": "2024-07-06T04:33:06.105018Z",
     "shell.execute_reply.started": "2024-07-06T04:33:06.094026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5498)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[1, :] @ v[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b3bd443-c1fc-4e8c-998e-fb56a541650c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:31:30.492776Z",
     "iopub.status.busy": "2024-07-06T04:31:30.492150Z",
     "iopub.status.idle": "2024-07-06T04:31:30.506917Z",
     "shell.execute_reply": "2024-07-06T04:31:30.504798Z",
     "shell.execute_reply.started": "2024-07-06T04:31:30.492732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4550, 3.6625])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exponential summation (softmax 的分母)\n",
    "torch.sum(torch.exp(s), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "199ec0a2-b0f1-4467-80b3-b56d34d652ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T04:34:02.518978Z",
     "iopub.status.busy": "2024-07-06T04:34:02.518358Z",
     "iopub.status.idle": "2024-07-06T04:34:02.530607Z",
     "shell.execute_reply": "2024-07-06T04:34:02.528405Z",
     "shell.execute_reply.started": "2024-07-06T04:34:02.518935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.74685641864268"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (9.5498 * 3.66 + 7.5498 * 2.455) / (2.455 + 3.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1aaaf8-84c8-4b5c-961d-6b46fd3378ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7297ff5c-91b5-48b0-8848-c5939745ea95",
   "metadata": {},
   "source": [
    "### SDPA（pytorch）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89af96-5de8-4075-bf7c-ce68dfe20de2",
   "metadata": {},
   "source": [
    "## simple demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3f09ab-3737-4f5a-a297-9f7836d412b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T03:14:49.241231Z",
     "iopub.status.busy": "2024-07-06T03:14:49.240622Z",
     "iopub.status.idle": "2024-07-06T03:14:49.266308Z",
     "shell.execute_reply": "2024-07-06T03:14:49.264262Z",
     "shell.execute_reply.started": "2024-07-06T03:14:49.241187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.99999999, 13.99999999, 14.99999999, 15.99999999],\n",
       "       [13.        , 14.        , 15.        , 16.        ],\n",
       "       [13.        , 14.        , 15.        , 16.        ],\n",
       "       [13.        , 14.        , 15.        , 16.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 输入矩阵\n",
    "X = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8],\n",
    "              [9, 10, 11, 12],\n",
    "              [13, 14, 15, 16]])\n",
    "\n",
    "# 权重矩阵\n",
    "W_Q = W_K = W_V = np.eye(4)\n",
    "\n",
    "# 经典自注意力机制\n",
    "Q = np.dot(X, W_Q)\n",
    "K = np.dot(X, W_K)\n",
    "V = np.dot(X, W_V)\n",
    "attention_scores = np.dot(Q, K.T) / np.sqrt(4)\n",
    "attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)\n",
    "output_classic = np.dot(attention_weights, V)\n",
    "\n",
    "output_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629cfa94-d2e1-4e0f-8e25-b8c79245453f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T03:14:54.410300Z",
     "iopub.status.busy": "2024-07-06T03:14:54.409717Z",
     "iopub.status.idle": "2024-07-06T03:14:54.430594Z",
     "shell.execute_reply": "2024-07-06T03:14:54.428471Z",
     "shell.execute_reply.started": "2024-07-06T03:14:54.410257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.99999999, 13.99999999, 14.99999999, 15.99999999],\n",
       "       [13.        , 14.        , 15.        , 16.        ],\n",
       "       [13.        , 14.        , 15.        , 16.        ],\n",
       "       [13.        , 14.        , 15.        , 16.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flash Attention\n",
    "b = 2\n",
    "m = 2\n",
    "output_flash = np.zeros((4, 4))\n",
    "for i in range(b):\n",
    "    X_block = X[i * m: (i + 1) * m]\n",
    "    Q_block = np.dot(X_block, W_Q)\n",
    "    K_block = np.dot(X_block, W_K)\n",
    "    V_block = np.dot(X_block, W_V)\n",
    "    \n",
    "    # 计算块间的注意力得分\n",
    "    attention_scores_block = np.dot(Q_block, K.T) / np.sqrt(4)\n",
    "    attention_weights_block = np.exp(attention_scores_block) / np.sum(np.exp(attention_scores_block), axis=1, keepdims=True)\n",
    "    \n",
    "    # 累加到输出\n",
    "    output_flash[i * m: (i + 1) * m] = np.dot(attention_weights_block, V)\n",
    "\n",
    "output_flash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
