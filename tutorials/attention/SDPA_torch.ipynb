{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f94bf3-26da-4976-85bf-a6258487c2bb",
   "metadata": {},
   "source": [
    "```\n",
    "ImportError: PyTorch SDPA requirements in Transformers are not met. Please install torch>=2.1.1.\n",
    "```\n",
    "\n",
    "- https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59c5a0c-1a27-4ef7-9c88-e6272dbc7074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:20:58.085693Z",
     "iopub.status.busy": "2024-07-07T09:20:58.084287Z",
     "iopub.status.idle": "2024-07-07T09:20:59.480181Z",
     "shell.execute_reply": "2024-07-07T09:20:59.479228Z",
     "shell.execute_reply.started": "2024-07-07T09:20:58.085636Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ac02c7-8351-4fcd-865e-4bba2ba9dec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:05:48.763026Z",
     "iopub.status.busy": "2024-07-02T15:05:48.762834Z",
     "iopub.status.idle": "2024-07-02T15:05:48.769584Z",
     "shell.execute_reply": "2024-07-02T15:05:48.768865Z",
     "shell.execute_reply.started": "2024-07-02T15:05:48.763013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db2751f-19a6-4448-be0c-7d1f879eb691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:05:48.772098Z",
     "iopub.status.busy": "2024-07-02T15:05:48.771972Z",
     "iopub.status.idle": "2024-07-02T15:05:48.976738Z",
     "shell.execute_reply": "2024-07-02T15:05:48.975924Z",
     "shell.execute_reply.started": "2024-07-02T15:05:48.772088Z"
    }
   },
   "outputs": [],
   "source": [
    "Q, K, V = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b2abcd4-739b-4f09-b865-eb70a00f5bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:05:48.977481Z",
     "iopub.status.busy": "2024-07-02T15:05:48.977339Z",
     "iopub.status.idle": "2024-07-02T15:05:49.101062Z",
     "shell.execute_reply": "2024-07-02T15:05:49.100255Z",
     "shell.execute_reply.started": "2024-07-02T15:05:48.977470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3321, -0.3489,  0.3015, -0.3912,  0.9867,  0.3137, -0.0691,\n",
       "          -1.2593],\n",
       "         [-1.0882,  0.2506,  0.6491,  0.1360,  0.5238, -0.2448, -0.0820,\n",
       "          -0.6171],\n",
       "         [-1.0012,  0.3990,  0.6441, -0.0277,  0.5325, -0.2564, -0.0607,\n",
       "          -0.6404]],\n",
       "\n",
       "        [[ 0.6091,  0.0708,  0.6188,  0.3252, -0.1598,  0.4197, -0.2335,\n",
       "           0.0630],\n",
       "         [ 0.5285,  0.3890, -0.2649,  0.3706, -0.3839,  0.1963, -0.6242,\n",
       "           0.2312],\n",
       "         [ 0.4048,  0.0762,  0.3777,  0.4689, -0.2978,  0.2754, -0.6429,\n",
       "           0.1037]]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = F.scaled_dot_product_attention(Q, K, V)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd841a4-31fb-48ef-afec-688885ca3530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:05:49.101951Z",
     "iopub.status.busy": "2024-07-02T15:05:49.101810Z",
     "iopub.status.idle": "2024-07-02T15:05:49.106562Z",
     "shell.execute_reply": "2024-07-02T15:05:49.105870Z",
     "shell.execute_reply.started": "2024-07-02T15:05:49.101940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe003f93-842c-4543-937b-10ea43bef115",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aef6873-4e1f-4ebc-a4cf-38fb0ce8c97a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:05:49.107070Z",
     "iopub.status.busy": "2024-07-02T15:05:49.106948Z",
     "iopub.status.idle": "2024-07-02T15:05:49.117089Z",
     "shell.execute_reply": "2024-07-02T15:05:49.116379Z",
     "shell.execute_reply.started": "2024-07-02T15:05:49.107061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.bmm(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(8)), dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "379a7c31-3351-4c43-a8d1-15c43543099e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:05:49.117574Z",
     "iopub.status.busy": "2024-07-02T15:05:49.117453Z",
     "iopub.status.idle": "2024-07-02T15:05:49.125447Z",
     "shell.execute_reply": "2024-07-02T15:05:49.124733Z",
     "shell.execute_reply.started": "2024-07-02T15:05:49.117565Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3321, -0.3489,  0.3015, -0.3912,  0.9867,  0.3137, -0.0691,\n",
       "          -1.2593],\n",
       "         [-1.0882,  0.2506,  0.6491,  0.1360,  0.5238, -0.2448, -0.0820,\n",
       "          -0.6171],\n",
       "         [-1.0012,  0.3990,  0.6441, -0.0277,  0.5325, -0.2564, -0.0607,\n",
       "          -0.6404]],\n",
       "\n",
       "        [[ 0.6091,  0.0708,  0.6188,  0.3252, -0.1598,  0.4197, -0.2335,\n",
       "           0.0630],\n",
       "         [ 0.5285,  0.3890, -0.2649,  0.3706, -0.3839,  0.1963, -0.6242,\n",
       "           0.2312],\n",
       "         [ 0.4048,  0.0762,  0.3777,  0.4689, -0.2978,  0.2754, -0.6429,\n",
       "           0.1037]]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(F.softmax(torch.bmm(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(8)), dim=-1), V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb1ef1-27ac-4999-a823-898fa6710f9e",
   "metadata": {},
   "source": [
    "## SDPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269f52b-c544-4667-b37e-09d691cb4b23",
   "metadata": {},
   "source": [
    "- The default implementation runs in 26186.948 microseconds\n",
    "- The math implementation runs in 50155.869 microseconds\n",
    "- The flash attention implementation runs in 26189.985 microseconds\n",
    "- The memory efficient implementation runs in 48395.111 microseconds\n",
    "- PyTorchâ€™s `torch.nn.functional.scaled_dot_product_attention` (SDPA) can also call `FlashAttention` and `memory-efficient attention kernels` under the hood. SDPA support is currently being added natively in Transformers and is used by default for torch>=2.1.1 when an implementation is available. You may also set attn_implementation=\"sdpa\" in from_pretrained() to explicitly request SDPA to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cfeb170-a94d-4b44-b677-50661f84997e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:21:05.944360Z",
     "iopub.status.busy": "2024-07-07T09:21:05.944038Z",
     "iopub.status.idle": "2024-07-07T09:21:06.741484Z",
     "shell.execute_reply": "2024-07-07T09:21:06.739794Z",
     "shell.execute_reply.started": "2024-07-07T09:21:05.944339Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "def benchmark_sdpa(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9193fa4d-c7ea-4c68-9e0b-2cfbd50b74fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:21:08.360812Z",
     "iopub.status.busy": "2024-07-07T09:21:08.360494Z",
     "iopub.status.idle": "2024-07-07T09:21:08.368465Z",
     "shell.execute_reply": "2024-07-07T09:21:08.366317Z",
     "shell.execute_reply.started": "2024-07-07T09:21:08.360792Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets define the hyper-parameters of our input\n",
    "bs = 32\n",
    "seq_len = 2*1024\n",
    "n_heads = 32\n",
    "embed_dimen = 224\n",
    "\n",
    "dtype = torch.float16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c68446-cf47-4d68-91c1-5dec446f0a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:21:12.015161Z",
     "iopub.status.busy": "2024-07-07T09:21:12.014535Z",
     "iopub.status.idle": "2024-07-07T09:21:12.186814Z",
     "shell.execute_reply": "2024-07-07T09:21:12.185430Z",
     "shell.execute_reply.started": "2024-07-07T09:21:12.015116Z"
    }
   },
   "outputs": [],
   "source": [
    "Q = torch.rand(bs, n_heads, seq_len, embed_dimen, device=device, dtype=dtype)\n",
    "K = torch.rand(bs, n_heads, seq_len, embed_dimen, device=device, dtype=dtype)\n",
    "V = torch.rand(bs, n_heads, seq_len, embed_dimen, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec84edf-b722-4814-919e-6f6d09b57dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:21:14.374750Z",
     "iopub.status.busy": "2024-07-07T09:21:14.374381Z",
     "iopub.status.idle": "2024-07-07T09:21:14.981921Z",
     "shell.execute_reply": "2024-07-07T09:21:14.980623Z",
     "shell.execute_reply.started": "2024-07-07T09:21:14.374726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The default implementation runs in 26101.244 microseconds'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"The default implementation runs in {benchmark_sdpa(F.scaled_dot_product_attention, Q, K, V):.3f} microseconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f001cf-ba79-4140-8c9b-cfc2f39bf0c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:21:16.761401Z",
     "iopub.status.busy": "2024-07-07T09:21:16.760192Z",
     "iopub.status.idle": "2024-07-07T09:21:17.462523Z",
     "shell.execute_reply": "2024-07-07T09:21:17.460381Z",
     "shell.execute_reply.started": "2024-07-07T09:21:16.761371Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.00 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SDPBackend, sdpa_kernel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sdpa_kernel(SDPBackend\u001b[38;5;241m.\u001b[39mMATH):\n\u001b[0;32m----> 4\u001b[0m     math_time\u001b[38;5;241m=\u001b[39m\u001b[43mbenchmark_sdpa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe math implementation runs in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m microseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mbenchmark_sdpa\u001b[0;34m(f, *args, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbenchmark_sdpa\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      3\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mTimer(\n\u001b[1;32m      4\u001b[0m         stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf(*args, **kwargs)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m: f}\n\u001b[1;32m      5\u001b[0m     )\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocked_autorange\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e6\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/benchmark/utils/timer.py:372\u001b[0m, in \u001b[0;36mTimer.blocked_autorange\u001b[0;34m(self, callback, min_run_time)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mblocked_autorange\u001b[39m(\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    333\u001b[0m     callback: Optional[Callable[[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], NoReturn]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    334\u001b[0m     min_run_time: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m    335\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m common\u001b[38;5;241m.\u001b[39mMeasurement:\n\u001b[1;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Measure many replicates while keeping timer overhead to a minimum.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m    At a high level, blocked_autorange executes the following pseudo-code::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m        (mean, median, etc.)\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_block_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_run_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtime_hook\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeit(number)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/benchmark/utils/timer.py:319\u001b[0m, in \u001b[0;36mTimer._estimate_block_size\u001b[0;34m(self, min_run_time)\u001b[0m\n\u001b[1;32m    317\u001b[0m number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     time_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     relative_overhead \u001b[38;5;241m=\u001b[39m overhead \u001b[38;5;241m/\u001b[39m time_taken\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m relative_overhead \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m time_taken \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_run_time \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/benchmark/utils/timer.py:264\u001b[0m, in \u001b[0;36mTimer._timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_timeit\u001b[39m(\u001b[38;5;28mself\u001b[39m, number: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# Even calling a timer in C++ takes ~50 ns, so no real operation should\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# take less than 1 ns. (And this prevents divide by zero errors.)\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1e-9\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/timeit.py:178\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    176\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.00 GiB. GPU "
     ]
    }
   ],
   "source": [
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "with sdpa_kernel(SDPBackend.MATH):\n",
    "    math_time=benchmark_sdpa(F.scaled_dot_product_attention, Q, K, V)\n",
    "    print(f\"The math implementation runs in {math_time:.3f} microseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a87f6899-c162-428f-8b7f-b18baeacecad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:05:58.633412Z",
     "iopub.status.busy": "2024-07-02T15:05:58.633077Z",
     "iopub.status.idle": "2024-07-02T15:05:59.205359Z",
     "shell.execute_reply": "2024-07-02T15:05:59.204730Z",
     "shell.execute_reply.started": "2024-07-02T15:05:58.633389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flash attention implementation runs in 26189.985 microseconds\n"
     ]
    }
   ],
   "source": [
    "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "    try:\n",
    "        flash_time=benchmark_sdpa(F.scaled_dot_product_attention, Q, K, V)\n",
    "        print(f\"The flash attention implementation runs in {flash_time:.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"FlashAttention is not supported. See warnings for reasons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fb0f6e8-9278-486e-b555-edc146bd456e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:06:53.362860Z",
     "iopub.status.busy": "2024-07-02T15:06:53.362229Z",
     "iopub.status.idle": "2024-07-02T15:06:53.650760Z",
     "shell.execute_reply": "2024-07-02T15:06:53.648993Z",
     "shell.execute_reply.started": "2024-07-02T15:06:53.362812Z"
    }
   },
   "outputs": [],
   "source": [
    "Q = torch.rand(bs, n_heads, seq_len, embed_dimen, device='cuda:1', dtype=dtype)\n",
    "K = torch.rand(bs, n_heads, seq_len, embed_dimen, device='cuda:1', dtype=dtype)\n",
    "V = torch.rand(bs, n_heads, seq_len, embed_dimen, device='cuda:1', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f7880a7-e404-42f5-8dbd-0fe6b8e197b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:06:54.949383Z",
     "iopub.status.busy": "2024-07-02T15:06:54.949085Z",
     "iopub.status.idle": "2024-07-02T15:07:47.613700Z",
     "shell.execute_reply": "2024-07-02T15:07:47.613137Z",
     "shell.execute_reply.started": "2024-07-02T15:06:54.949363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory efficient implementation runs in 48395.111 microseconds\n"
     ]
    }
   ],
   "source": [
    "with sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION):\n",
    "    try:\n",
    "        efficient_time=benchmark_sdpa(F.scaled_dot_product_attention, Q, K, V)\n",
    "        print(f\"The memory efficient implementation runs in {efficient_time:.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"EfficientAttention is not supported. See warnings for reasons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ddbdf-b0f2-496a-8b12-3c58ca1b706f",
   "metadata": {},
   "source": [
    "## Causal Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14fcecc-d9c7-4247-abd4-bb242e916db6",
   "metadata": {},
   "source": [
    "- https://github.com/karpathy/nanoGPT/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd3030e9-43c0-4bac-b61d-e96061f7a765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:22:17.066261Z",
     "iopub.status.busy": "2024-07-02T15:22:17.065614Z",
     "iopub.status.idle": "2024-07-02T15:22:17.075431Z",
     "shell.execute_reply": "2024-07-02T15:22:17.073311Z",
     "shell.execute_reply.started": "2024-07-02T15:22:17.066214Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_dim = 12\n",
    "n_heads = 2\n",
    "assert embed_dim % n_heads == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e5240a0-29bf-4cb1-9f2f-3946f56dcdcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:22:18.856356Z",
     "iopub.status.busy": "2024-07-02T15:22:18.855662Z",
     "iopub.status.idle": "2024-07-02T15:22:18.868368Z",
     "shell.execute_reply": "2024-07-02T15:22:18.866268Z",
     "shell.execute_reply.started": "2024-07-02T15:22:18.856306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_dim = embed_dim // (n_heads * 3)\n",
    "head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58d5121e-6686-4746-8f18-ddbdea779e52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:19:31.167315Z",
     "iopub.status.busy": "2024-07-02T15:19:31.166646Z",
     "iopub.status.idle": "2024-07-02T15:19:31.176478Z",
     "shell.execute_reply": "2024-07-02T15:19:31.174473Z",
     "shell.execute_reply.started": "2024-07-02T15:19:31.167266Z"
    }
   },
   "outputs": [],
   "source": [
    "# W_q, W_k, W_v\n",
    "c_attn = nn.Linear(embed_dim, 3 * embed_dim, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76bf52a5-2a82-4527-bd91-932dfd35a595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:20:31.611245Z",
     "iopub.status.busy": "2024-07-02T15:20:31.610596Z",
     "iopub.status.idle": "2024-07-02T15:20:31.619783Z",
     "shell.execute_reply": "2024-07-02T15:20:31.617774Z",
     "shell.execute_reply.started": "2024-07-02T15:20:31.611199Z"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.randn(2, 5, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da2bea94-846c-4957-acbb-82279104f529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:20:33.388661Z",
     "iopub.status.busy": "2024-07-02T15:20:33.388036Z",
     "iopub.status.idle": "2024-07-02T15:20:33.401354Z",
     "shell.execute_reply": "2024-07-02T15:20:33.399098Z",
     "shell.execute_reply.started": "2024-07-02T15:20:33.388615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 36])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKV = c_attn(X)\n",
    "QKV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c043d777-c0de-4368-9a25-ea37ead32aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:20:34.961676Z",
     "iopub.status.busy": "2024-07-02T15:20:34.960961Z",
     "iopub.status.idle": "2024-07-02T15:20:34.971316Z",
     "shell.execute_reply": "2024-07-02T15:20:34.969451Z",
     "shell.execute_reply.started": "2024-07-02T15:20:34.961625Z"
    }
   },
   "outputs": [],
   "source": [
    "Q, K, V = QKV.chunk(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c7bfe56-f745-4d6b-96f9-331695c484b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:20:36.556381Z",
     "iopub.status.busy": "2024-07-02T15:20:36.555665Z",
     "iopub.status.idle": "2024-07-02T15:20:36.568763Z",
     "shell.execute_reply": "2024-07-02T15:20:36.566495Z",
     "shell.execute_reply.started": "2024-07-02T15:20:36.556333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(QKV[:, :, :12], Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08163375-f669-4822-857a-5ac853a49b49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:20:38.004913Z",
     "iopub.status.busy": "2024-07-02T15:20:38.004275Z",
     "iopub.status.idle": "2024-07-02T15:20:38.016953Z",
     "shell.execute_reply": "2024-07-02T15:20:38.014655Z",
     "shell.execute_reply.started": "2024-07-02T15:20:38.004867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 12])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dff3f92f-2959-4df5-b458-f9c4b576ce7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:23:07.819346Z",
     "iopub.status.busy": "2024-07-02T15:23:07.818703Z",
     "iopub.status.idle": "2024-07-02T15:23:07.830667Z",
     "shell.execute_reply": "2024-07-02T15:23:07.828645Z",
     "shell.execute_reply.started": "2024-07-02T15:23:07.819300Z"
    }
   },
   "outputs": [],
   "source": [
    "Q = Q.view(1, -1, n_heads, embed_dim//n_heads).transpose(1, 2)\n",
    "K = K.view(1, -1, n_heads, embed_dim//n_heads).transpose(1, 2)\n",
    "V = V.view(1, -1, n_heads, embed_dim//n_heads).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb70491-f50c-4865-83bb-3ed7ee440d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
