{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168e5d1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:14:58.312808Z",
     "start_time": "2023-07-05T13:14:58.306615Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede970ba",
   "metadata": {},
   "source": [
    "## trainer arguments & trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc781edd",
   "metadata": {},
   "source": [
    "- $q(x)$：from student model，$p(x)$：from teacher model\n",
    "- 其次对于 $q(x), p(x)$ 在计算时需要加温度\n",
    "$$\n",
    "\\begin{split}\n",
    "L_{\\text{student}}&=\\alpha L_{\\text{CE}} + (1-\\alpha)L_{KD}\\\\\n",
    "&=\\alpha L_{\\text{CE}} + (1-\\alpha)T^2D_{KL}\\\\\n",
    "&=\\alpha L_{\\text{CE}} + (1-\\alpha)T^2\\sum_ip_i(x)\\log\\frac{p_i(x)}{q_i(x)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- 关于 `nn.KLDivLoss()`\n",
    "    - inputs ($q(x)$): log probabilities\n",
    "    - labels ($p(x)$): normal probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf76fa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:15:02.457578Z",
     "start_time": "2023-07-05T13:14:59.752244Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59dde685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:15:32.807558Z",
     "start_time": "2023-07-05T13:15:32.801471Z"
    }
   },
   "outputs": [],
   "source": [
    "class DistillTrainingArguments(TrainingArguments):\n",
    "    # TrainingArguments: @dataclass\n",
    "    # 增加两个 KD 所需的参数参数\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2., **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35763030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:16:05.261323Z",
     "start_time": "2023-07-05T13:16:05.246250Z"
    }
   },
   "outputs": [],
   "source": [
    "class DistillTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        # 增加 teacher_model 参数\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        \n",
    "    # 重写 trainer 中核心方法\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        s_output = model(**inputs)\n",
    "        s_ce = s_output.loss\n",
    "        s_logits = s_output.logits\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            t_output = self.teacher_model(**inputs)\n",
    "            t_logits = t_output.logits\n",
    "        \n",
    "        loss_kl_fct = nn.KLDivLoss(reduction='batchmean')\n",
    "        loss_kd = self.args.temperature**2 * loss_kl_fct(F.log_softmax(s_logits/self.args.temperature, dim=-1), \n",
    "                                                        F.softmax(t_logits/self.args.temperature, dim=-1))\n",
    "        loss = self.args.alpha * s_ce + (1-self.args.alpha) * loss_kd\n",
    "        return (loss, s_output) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd85c3",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2702dd",
   "metadata": {},
   "source": [
    "### datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b79b20b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:16:08.110824Z",
     "start_time": "2023-07-05T13:16:07.837209Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/whaow/.cache/huggingface/modules/datasets_modules/datasets/clinc_oos/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1 (last modified on Wed Jun 14 00:03:25 2023) since it couldn't be found locally at clinc_oos., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset clinc_oos (/home/whaow/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe498b7907ec497f9a4455e3ee97cfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c17cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:16:09.689665Z",
     "start_time": "2023-07-05T13:16:09.679476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'intent'],\n",
       "        num_rows: 15250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'intent'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'intent'],\n",
       "        num_rows: 5500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f258185",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:23:49.001606Z",
     "start_time": "2023-07-05T13:23:48.989616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['what expression would i use to say i love you if i were an italian',\n",
       "  \"can you tell me how to say 'i do not speak much spanish', in spanish\",\n",
       "  \"what is the equivalent of, 'life is good' in french\",\n",
       "  \"tell me how to say, 'it is a beautiful morning' in italian\",\n",
       "  'if i were mongolian, how would i say that i am a tourist',\n",
       "  \"how do i say 'hotel' in finnish\",\n",
       "  \"i need you to translate the sentence, 'we will be there soon' into portuguese\",\n",
       "  'please tell me how to ask for a taxi in french',\n",
       "  \"can you tell me how i would say, 'more bread please' in french\",\n",
       "  \"what is the correct way to say 'i am a visitor' in french\"],\n",
       " 'intent': [61, 61, 61, 61, 61, 61, 61, 61, 61, 61]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinc['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4366a862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:18:31.711384Z",
     "start_time": "2023-07-05T13:18:31.705706Z"
    }
   },
   "outputs": [],
   "source": [
    "intents = clinc['train'].features['intent']\n",
    "num_labels = intents.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4a0a9",
   "metadata": {},
   "source": [
    "### Student model 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313668df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:17:18.184748Z",
     "start_time": "2023-07-05T13:17:18.177111Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a68a82b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:30:31.641791Z",
     "start_time": "2023-07-05T13:30:29.397430Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "s_ckpt = 'distilbert-base-uncased'\n",
    "s_tokenizer = AutoTokenizer.from_pretrained(s_ckpt)\n",
    "\n",
    "t_ckpt = 'transformersbook/bert-base-uncased-finetuned-clinc'\n",
    "t_model = AutoModelForSequenceClassification.from_pretrained(t_ckpt, num_labels=num_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30765bdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:21:23.324349Z",
     "start_time": "2023-07-05T13:21:22.881717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/whaow/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1/cache-f9c6f4c987e9be44.arrow\n",
      "Loading cached processed dataset at /home/whaow/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1/cache-83787f330c65b9f8.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 15250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinc_enc = clinc.map(lambda batch: s_tokenizer(batch['text'], truncation=True), \n",
    "                      batched=True, \n",
    "                      remove_columns=[\"text\"]\n",
    "                     )\n",
    "clinc_enc = clinc_enc.rename_columns({'intent': 'labels'})\n",
    "clinc_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9a08ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:44:10.327525Z",
     "start_time": "2023-07-05T13:44:09.937677Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "s_training_args = DistillTrainingArguments(output_dir='distilbert-base-uncased-ft-clinc', \n",
    "                                           evaluation_strategy='epoch', num_train_epochs=5, \n",
    "                                           learning_rate=3e-4, \n",
    "                                           per_device_train_batch_size=batch_size, \n",
    "                                           per_device_eval_batch_size=batch_size, \n",
    "                                           alpha=1, weight_decay=0.01, \n",
    "                                           logging_strategy='epoch',\n",
    "                                           push_to_hub=True)\n",
    "s_config = AutoConfig.from_pretrained(s_ckpt, num_labels=num_labels, \n",
    "                                      id2label=t_model.config.id2label, label2id=t_model.config.label2id)\n",
    "# s_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86ba1a09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:25:52.142566Z",
     "start_time": "2023-07-05T13:25:52.134569Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def student_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(s_ckpt, config=s_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294067e",
   "metadata": {},
   "source": [
    "### trainer.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8026a1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:26:02.058387Z",
     "start_time": "2023-07-05T13:25:57.513746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/whaow/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Sun Jun 25 23:06:51 2023) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_metric\n",
    "# accuracy_score = load_metric('accuracy')\n",
    "# SequenceClassification\n",
    "import evaluate\n",
    "accuracy_score = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d9f99a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:44:14.230476Z",
     "start_time": "2023-07-05T13:44:14.220977Z"
    }
   },
   "outputs": [],
   "source": [
    "# trainer 重要的回调函数，非成员函数\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return accuracy_score.compute(references=labels, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c635f0b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:46:26.679761Z",
     "start_time": "2023-07-05T13:45:16.366623Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/whaow/workspaces/bert_t5_gpt/tutorials/distilbert-base-uncased-ft-clinc is already a clone of https://huggingface.co/lanchunhui/distilbert-base-uncased-ft-clinc. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 01:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.765900</td>\n",
       "      <td>0.369926</td>\n",
       "      <td>0.912903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.278254</td>\n",
       "      <td>0.936452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.295684</td>\n",
       "      <td>0.939032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.239457</td>\n",
       "      <td>0.947097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.234831</td>\n",
       "      <td>0.947742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.4035262276728948, metrics={'train_runtime': 66.1569, 'train_samples_per_second': 1152.563, 'train_steps_per_second': 9.069, 'total_flos': 456233053284036.0, 'train_loss': 0.4035262276728948, 'epoch': 5.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_trainer = DistillTrainer(model_init=student_init, teacher_model=t_model, args=s_training_args, \n",
    "                                 train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'], \n",
    "                                 compute_metrics=compute_metrics, tokenizer=s_tokenizer)\n",
    "distill_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "820b9f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:33:13.799230Z",
     "start_time": "2023-07-05T13:33:13.789610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil(15250/(64*2)) * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c3a00",
   "metadata": {},
   "source": [
    "### 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69a441ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:51:23.505698Z",
     "start_time": "2023-07-05T13:51:23.498648Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6a527d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T13:51:37.816702Z",
     "start_time": "2023-07-05T13:51:24.551236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "fatal: unable to access 'https://huggingface.co/lanchunhui/distilbert-base-uncased-ft-clinc/': gnutls_handshake() failed: Error in the pull function.\n",
      "\n",
      "Error pushing update to the model card. Please read logs and retry.\n",
      "$fatal: unable to access 'https://huggingface.co/lanchunhui/distilbert-base-uncased-ft-clinc/': gnutls_handshake() failed: Error in the pull function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ft_ckpt = 'lanchunhui/distilbert-base-uncased-ft-clinc'\n",
    "distill_trainer.push_to_hub('finetune completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8705e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
