{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e01ee35-a47a-4a45-a1c5-6456fd2520c6",
   "metadata": {},
   "source": [
    "- 如何评价一个模型的输出是不是瞎猜：PPL 指标；\n",
    "- 重新 review GPT 的过程\n",
    "    - input_ids: 1*1024, 一个（bs）长度为 1024 的 token ids\n",
    "    - last_hidden_states: 1\\*1024\\*768\n",
    "        - last layer hidden states of (transformer)\n",
    "    - lm_logits: 1\\*1024\\*50257\n",
    "        - lm head，将每一个位置上的 token 的 hidden state，映射到整个词表维度上的概率分布输出\n",
    "- shift labels 与损失计算\n",
    "\n",
    "    ```\n",
    "    labels = labels.to(lm_logits.device)\n",
    "    \n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    # Flatten the tokens\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f242c9-ba84-42c2-ad41-e904f4c5a228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T15:20:58.914000Z",
     "iopub.status.busy": "2024-06-20T15:20:58.913405Z",
     "iopub.status.idle": "2024-06-20T15:20:58.924353Z",
     "shell.execute_reply": "2024-06-20T15:20:58.922097Z",
     "shell.execute_reply.started": "2024-06-20T15:20:58.913956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5] [1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5] [2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "logits = [1, 2, 3, 4, 5]\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "print(logits, logits[:-1])\n",
    "print(labels, labels[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc43c8-60f1-4644-977e-bddcb37d9a2c",
   "metadata": {},
   "source": [
    "## casual/decoder only 单向注意力的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e5c8b7-817b-4295-b2fa-f443885dbf31",
   "metadata": {},
   "source": [
    "\n",
    "- modeling_gpt2.py\n",
    "    - GPT2Attention._attn\n",
    "\n",
    "```\n",
    "if not self.is_cross_attention:\n",
    "    # if only \"normal\" attention layer implements causal mask\n",
    "    query_length, key_length = query.size(-2), key.size(-2)\n",
    "    causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "    mask_value = torch.finfo(attn_weights.dtype).min\n",
    "    # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "    # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "    mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
    "    attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
    "\n",
    "if attention_mask is not None:\n",
    "    # Apply the attention mask\n",
    "    attn_weights = attn_weights + attention_mask\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
